
# YAGO part - worked
gcloud beta dataproc clusters create st446-mycluster --properties="^#^spark:spark.jars.packages=graphframes:graphframes:0.5.0-spark2.1-s_2.11,com.databricks:spark-xml_2.11:0.4.1" --subnet default --enable-component-gateway --region europe-west2 --zone europe-west2-c --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 1.3-debian10 --optional-components ANACONDA,JUPYTER --project st446-2021-lt --metadata "PIP_PACKAGES=sklearn nltk pandas graphframes" --project st446-lse-2021 --bucket st446-mybucket

# LDA part
gcloud beta dataproc clusters create st446-mycluster-lda --project st446-lse-2021 --bucket st446-mybucket --region europe-west2 --zone europe-west2-c --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version=1.4-debian10 --optional-components=ANACONDA,JUPYTER --enable-component-gateway --initialization-actions gs://goog-dataproc-initialization-actions-europe-west2/python/pip-install.sh --metadata "PIP_PACKAGES=sklearn nltk pandas numpy"

# LDA 2
gcloud beta dataproc clusters create mycluster-lda2 --enable-component-gateway --bucket st446-mybucket --region europe-west2 --zone europe-west2-b --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 1.4-debian10 --optional-components ANACONDA,JUPYTER --initialization-actions 'gs://goog-dataproc-initialization-actions-europe-west2/python/pip-install.sh','gs://st446-mybucket/my-actions.sh' --metadata PIP_PACKAGES=sklearn nltk pandas numpy --project st446-lse-2021

# for LDA part - tried 28 March -works
# first upload my-actions.sh file
gcloud beta dataproc clusters create mycluster-lda --project st446-lse-2021 \
    --bucket st446-mybucket --region europe-west2 \
    --master-machine-type n1-standard-4 --master-boot-disk-size 500 \
    --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 250 \
    --image-version=1.4-debian10 \
    --optional-components=ANACONDA,JUPYTER \
    --enable-component-gateway \
    --initialization-actions \
    gs://goog-dataproc-initialization-actions-europe-west2/python/pip-install.sh,gs://st446-mybucket/my-actions.sh \
    --metadata 'PIP_PACKAGES=sklearn nltk pandas numpy'


# Kafka streaming
gcloud beta dataproc clusters create mycluster-stream --project st446-lse-2021 \
 --enable-component-gateway \
 --region europe-west2 \
 --subnet default --zone europe-west2-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 0 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 \
 --image-version 1.3-deb9 \
 --initialization-actions \
 gs://dataproc-initialization-actions/jupyter/jupyter.sh,gs://dataproc-initialization-actions/python/pip-install.sh,gs://dataproc-initialization-actions/zookeeper/zookeeper.sh,gs://dataproc-initialization-actions/kafka/kafka.sh,gs://st446-mybucket/my-actions.sh \
 --metadata 'PIP_PACKAGES=sklearn nltk pandas graphframes pyspark kafka-python tweepy'


# TAB0: start kafka
cd /usr/lib/kafka
sudo -s
bin/kafka-server-start.sh config/server.properties &

# TAB1: create topic and then run terminal producer
cd /usr/lib/kafka
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic twitter-stream
bin/kafka-topics.sh --list --zookeeper localhost:2181

# TAB2: run kafka_twitter_pyspark
cd $SPARK_HOME
unset PYSPARK_DRIVER_PYTHON
bin/spark-submit ~/kafka_twitter_pyspark.py
