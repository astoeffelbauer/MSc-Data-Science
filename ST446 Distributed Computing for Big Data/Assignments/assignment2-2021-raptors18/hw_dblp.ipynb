{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2MAGZ2L7NJj"
   },
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "## Assignment 2 - PART 3\n",
    "---\n",
    "\n",
    "## P3: Topic Modelling\n",
    "\n",
    "In this homework problem, you are asked to perform a semantic analysis of the DBLP author publications dataset `dblp/author_large.txt` that you have already encountered before. *You may use GCP or your own computer. Please document your steps. The necessary initialisation actions for `nltk` are provided as part of week 9's class material.*\n",
    " \n",
    "**P3.A (10 points)** Use Latent Dirichlet Allocation (LDA) to cluster publications by using words in their titles and represent each publication by 10 topics. Please follow these steps:\n",
    "\n",
    "**1.** Convert titles to tokens by:\n",
    "   * Tokenizing words in the title of each publication\n",
    "   * Removing stop words using the `nltk` package\n",
    "   * Removing puctuations, numbers or other symbols\n",
    "   * Lemmatizing tokens\n",
    "\n",
    "Note that you may skip some of these editing steps or add some additional steps to edit the tokens, but if you do this provide a justification for it.\n",
    "\n",
    "**2.** Convert tokens into sparse vectors\n",
    "\n",
    "**3.** Use LDA to find out 10 topics for each publication and represent each topic with the first few most relevant words. Note that you can choose to use different number of topics rather than 10. Again if you do so, please provide a justification.\n",
    "\n",
    "**4.** Comment the obtained results.\n",
    "\n",
    "**P3.B (15 points)** Address each question/step as in part A, with the following modifications:\n",
    "\n",
    "**1.** Each *document* representing all publication tiles of a specific author. For example, if an author $Y$ wrote \"introduction to databases\" and \"database design\", then the *document* for the author $Y$ will be \"introduction to database database design\". \n",
    "\n",
    "**2.** In addition, calculate the topic density vector for each author and use the topic density to calculate the cosine similarity for each pair of authors. For example, if the topic density for author X is $[x_1, x_2, x_3, \\dots]$ and topic density vector for author Y is $[y_1, y_2, y_3, \\dots]$, then the cosine similarity is $\\frac{x_1\\cdot y_1 + x_2\\cdot y_2 + x_3\\cdot y_3 +\\dots}{\\sqrt{x_1^2+ x_2^2+ x_3^2 +\\dots}\\sqrt{y_1^2+ y_2^2+ y_3^2 +\\dots}}$. \n",
    "\n",
    "**3.** Show the 10 most similar author pairs and comment on their similarity, if possible taking into consideration the results from the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJa1YyUG7NJl"
   },
   "source": [
    "## 0. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster configuration\n",
    "```shell\n",
    "gcloud beta dataproc clusters create mycluster-lda --project st446-lse-2021 \\\n",
    "    --bucket st446-mybucket --region europe-west2 \\\n",
    "    --master-machine-type n1-standard-4 --master-boot-disk-size 500 \\\n",
    "    --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 250 \\\n",
    "    --image-version=1.4-debian10 \\\n",
    "    --optional-components=ANACONDA,JUPYTER \\\n",
    "    --enable-component-gateway \\\n",
    "    --initialization-actions \\\n",
    "    gs://goog-dataproc-initialization-actions-europe-west2/python/pip-install.sh,gs://st446-mybucket/my-actions.sh \\\n",
    "    --metadata 'PIP_PACKAGES=sklearn nltk pandas numpy'\n",
    "```\n",
    "\n",
    "\n",
    "#### Cluster shell commands\n",
    "```shell\n",
    "wget http://webdam.inria.fr/Jorge/files/author-large.txt\n",
    "gsutil cp author-large.txt gs://st446-mybucket/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KfcsGHg_7NJm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# your code to adjust the path to your dataset author-large.txt\n",
    "author_rdd = sc.textFile('gs://st446-mybucket/author-large.txt', 4) \\\n",
    "                .map(lambda row: np.array(row.strip().split(\"\\t\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZtDwN5BV7NJm"
   },
   "outputs": [],
   "source": [
    "# example - you can adjust for your case\n",
    "authors = author_rdd.map(lambda r: (r[0],1)).reduceByKey(lambda a,b: a+b)\n",
    "author_30 = set(authors.filter(lambda r: r[1] >= 30).map(lambda r: r[0]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V4jrVTZm7NJn",
    "outputId": "14604497-ec76-4419-d9e7-7edf51e417e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hector Garcia-Molina', 'Distributed Databases.'),\n",
       " ('Meichun Hsu', 'Distributed Databases.'),\n",
       " ('Nathan Goodman',\n",
       "  'An Object-Oriented DBMS War Story: Developing a Genome Mapping Database in C++.'),\n",
       " ('Gail E. Kaiser', 'Cooperative Transactions for Multiuser Environments.'),\n",
       " ('Guido Moerkotte', 'Physical Object Management.'),\n",
       " ('Won Kim', 'On Resolving Schematic Heterogeneity in Multidatabase Systems.'),\n",
       " ('Won Kim',\n",
       "  'Requirements for a Performance Benchmark for Object-Oriented Database Systems.'),\n",
       " ('Weiyi Meng', 'Query Processing in Multidatabase Systems.'),\n",
       " ('Clement T. Yu', 'Query Processing in Multidatabase Systems.'),\n",
       " ('Hanan Samet', 'Spatial Data Structures.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_author = author_rdd.filter(lambda r: r[0] in author_30). \\\n",
    "                    map(lambda r: (r[0],r[2])).distinct()\n",
    "title_author.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "97acIVei7NJo",
    "outputId": "aeecc32d-4371-4cde-9b13-0ec4079d9d01"
   },
   "outputs": [],
   "source": [
    "# print(author_rdd.count())\n",
    "# print(title_author.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An Adaptive Limited Pointers Directory Scheme for Cache Coherence of Scalable Multiprocessors.',\n",
       " 'Real PRAM Programming.',\n",
       " 'Design of Multi-dimensional DCT Array Processors for Video Applications.',\n",
       " 'Topic Introduction.',\n",
       " 'Topic Introduction.',\n",
       " 'Topic Introduction.',\n",
       " 'Efficient Total-Exchange in Wormhole-Routed Toroidal Cubes.',\n",
       " 'Comparison of Different Methods for Next Location Prediction.',\n",
       " 'Topic 12 Theory and Algorithms for Parallel Computation.',\n",
       " 'Topic 14 - Mobile and Ubiquitous Computing.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = title_author.map(lambda line: line[1])\n",
    "titles.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHwbHeY77NJo"
   },
   "source": [
    "## A1. Parse the data\n",
    "\n",
    "Here we make use of the natural language processing module `nltk`. Please download both the module and the corresponding data. See https://www.nltk.org/install.html and https://www.nltk.org/data.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6M_qGmj-7NJp"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6M_qGmj-7NJp"
   },
   "outputs": [],
   "source": [
    "# download stopwords and whatever\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('all', '/home/nltk_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6M_qGmj-7NJp"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lmtzr = WordNetLemmatizer() # see https://www.nltk.org/_modules/nltk/stem/wordnet.html for details\n",
    "\n",
    "def get_tokens(line):\n",
    "#     words = line.split(' ')\n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuations from each word\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # lemmatizing the words, see https://en.wikipedia.org/wiki/Lemmatisation\n",
    "    words = [lmtzr.lemmatize(w) for w in words]\n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6M_qGmj-7NJp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Distributed Databases.', ['distributed', 'database']),\n",
       " ('Distributed Databases.', ['distributed', 'database']),\n",
       " ('An Object-Oriented DBMS War Story: Developing a Genome Mapping Database in C++.',\n",
       "  ['objectoriented',\n",
       "   'dbms',\n",
       "   'war',\n",
       "   'story',\n",
       "   'developing',\n",
       "   'genome',\n",
       "   'mapping',\n",
       "   'database',\n",
       "   'c']),\n",
       " ('Cooperative Transactions for Multiuser Environments.',\n",
       "  ['cooperative', 'transaction', 'multiuser', 'environment']),\n",
       " ('Physical Object Management.', ['physical', 'object', 'management'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_rdd = title_author.map(lambda line: (str(line[1]), get_tokens(line[1])))\n",
    "titles_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the `get_tokens` function from week 9's computer class to tokenize the publication titles, remove stopwords and lemmatize words. The authors are not important for this part, so I discard them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4kE5SGl7NJp"
   },
   "source": [
    "## A2. Convert tokens into sparse vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|               words|\n",
      "+--------------------+--------------------+\n",
      "|An Adaptive Limit...|[adaptive, limite...|\n",
      "|Real PRAM Program...|[real, pram, prog...|\n",
      "|Design of Multi-d...|[design, multidim...|\n",
      "| Topic Introduction.|[topic, introduct...|\n",
      "| Topic Introduction.|[topic, introduct...|\n",
      "| Topic Introduction.|[topic, introduct...|\n",
      "|Efficient Total-E...|[efficient, total...|\n",
      "|Comparison of Dif...|[comparison, diff...|\n",
      "|Topic 12 Theory a...|[topic, theory, a...|\n",
      "|Topic 14 - Mobile...|[topic, mobile, u...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles_df = spark.createDataFrame(titles_rdd, [\"title\",\"words\"])\n",
    "titles_df.cache()\n",
    "titles_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjP2RHegERud"
   },
   "source": [
    "## Generate a vectorized representation of the *tokens*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               title|               words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|An Adaptive Limit...|[adaptive, limite...|(56322,[40,55,139...|\n",
      "|Real PRAM Program...|[real, pram, prog...|(56322,[54,361,28...|\n",
      "|Design of Multi-d...|[design, multidim...|(56322,[8,10,36,1...|\n",
      "| Topic Introduction.|[topic, introduct...|(56322,[464,582],...|\n",
      "| Topic Introduction.|[topic, introduct...|(56322,[464,582],...|\n",
      "| Topic Introduction.|[topic, introduct...|(56322,[464,582],...|\n",
      "|Efficient Total-E...|[efficient, total...|(56322,[17,1092,4...|\n",
      "|Comparison of Dif...|[comparison, diff...|(56322,[22,167,19...|\n",
      "|Topic 12 Theory a...|[topic, theory, a...|(56322,[6,25,128,...|\n",
      "|Topic 14 - Mobile...|[topic, mobile, u...|(56322,[28,49,410...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=2)\n",
    "\n",
    "cv_model = cv.fit(titles_df)\n",
    "\n",
    "titles_df_w_features = cv_model.transform(titles_df)\n",
    "titles_df_w_features.cache()\n",
    "titles_df_w_features.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a Spark dataframe, I use the `CountVectorizer` to turn the tokens into feature vectors. By specifying `minDF=2`, I only use tokens that appear in at least two different titles. \n",
    "\n",
    "The feature vectors are represented as a tiple of length 2 that contains the number of tokens as its first element as well as a list of all tokens present in a title as its second element (the tokens are encoded as integers). Overall, I use 56,322 unique tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxS38Bsj7NJq"
   },
   "source": [
    "## Convert pyspark.ml vectors to pyspark.mllib vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "I perform these updates but will not further use the resulting mlib feature vectors since the topic modelling is done using the newer ML library and not the older MLIB library. In addition, the feature vectors that result from the `CountVectorizer` are already sparse. Therefore, I am usure about the advantage of this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(56322, {12: 1.0, 23: 1.0})]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def as_mllib_vector(v):\n",
    "    return Vectors.sparse(v.size, v.indices, v.values)\n",
    "\n",
    "features = titles_df_w_features.select(\"features\")\n",
    "feature_vec = features.rdd.map(lambda r: as_mllib_vector(r[0]))\n",
    "\n",
    "# feature_vec.cache()\n",
    "feature_vec.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0JxSajd7NJr"
   },
   "source": [
    "## Check the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms W =  56322\n",
      "Vocabulary from CountVectorizerModel is:\n",
      "['system', 'using', 'network', 'model', 'based', 'data', 'algorithm', 'analysis', 'application', 'approach', 'design', 'image', 'distributed', 'web', 'service', 'learning', 'information', 'efficient', 'dynamic', 'architecture', 'performance', 'software', 'method', 'database', 'environment', 'parallel', 'control', 'management', 'mobile', 'framework', 'logic', 'query', 'problem', 'detection', 'object', 'modeling', 'video', 'wireless', 'language', 'evaluation', 'adaptive', 'new', 'protocol', 'graph', 'sensor', 'technique', 'process', 'program', 'optimization', 'computing', 'retrieval', 'agent', 'study', 'search', 'programming', 'scheme', 'support', 'scheduling', 'realtime', 'recognition', 'towards', 'semantic', 'communication', 'knowledge', 'mining', 'pattern', 'time', 'simulation', 'implementation', 'multiple', 'structure', 'constraint', 'automatic', 'classification', 'grid', 'routing', 'feature', 'power', 'memory', 'tool', 'generation', 'estimation', 'tree', 'user', 'fast', 'processing', 'virtual', 'robot', 'set', 'clustering', 'development', 'case', 'abstract', 'selection', 'machine', 'access', 'test', 'document', 'security', 'motion']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of terms W = \", len(cv_model.vocabulary))\n",
    "print (\"Vocabulary from CountVectorizerModel is:\")\n",
    "print(cv_model.vocabulary[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHmr73aL7NJr"
   },
   "source": [
    "## A3. Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.9 ms, sys: 4.82 ms, total: 29.7 ms\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialize and run LDA\n",
    "lda = LDA(k=10, maxIter=5)\n",
    "lda_model = lda.fit(titles_df_w_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9roli0F7NJs"
   },
   "source": [
    "Looking at the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z-D8pLq07NJr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|     [1, 3, 4, 6, 2]|[0.01304233675660...|\n",
      "|    1|[0, 703, 479, 17,...|[0.00220526495685...|\n",
      "|    2|  [30, 15, 47, 1, 4]|[0.00549904050926...|\n",
      "|    3|   [5, 0, 38, 64, 2]|[0.01474830098483...|\n",
      "|    4|   [2, 0, 1, 22, 18]|[0.00172715391860...|\n",
      "|    5| [13, 21, 14, 6, 61]|[9.99265139795183...|\n",
      "|    6|   [2, 0, 37, 42, 5]|[0.01509815541227...|\n",
      "|    7|     [4, 0, 2, 8, 5]|[0.00202393271187...|\n",
      "|    8|    [0, 1, 7, 12, 8]|[0.02100461290465...|\n",
      "|    9| [0, 7, 8, 1926, 43]|[0.00137709744974...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "topics = lda_model.describeTopics(5)\n",
    "\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "z-D8pLq07NJr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using' 'model' 'based' 'algorithm' 'network']\n",
      "['system' 'der' 'und' 'efficient' 'von']\n",
      "['logic' 'learning' 'program' 'using' 'based']\n",
      "['data' 'system' 'language' 'mining' 'network']\n",
      "['network' 'system' 'using' 'method' 'dynamic']\n",
      "['web' 'software' 'service' 'algorithm' 'semantic']\n",
      "['network' 'system' 'wireless' 'protocol' 'data']\n",
      "['based' 'system' 'network' 'application' 'data']\n",
      "['system' 'using' 'analysis' 'distributed' 'application']\n",
      "['system' 'analysis' 'application' 'di' 'graph']\n"
     ]
    }
   ],
   "source": [
    "# Shows the results\n",
    "import numpy as np\n",
    "topic_i = topics.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "for i in topic_i:\n",
    "    print(np.array(cv_model.vocabulary)[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5jJOFTKGWSe"
   },
   "source": [
    "## A4. Comment your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sticked with 10 different topics as suggested in the instructions. Some are, however, quite difficult to distinguish by their top 5 words only. Most likely, this is because many of these words are quite generic and frequently occur in titles across domains.\n",
    "\n",
    "One solution for this problem could be to remove more words, especially those that are very frequent across all publication titles. Alternatively, weighting terms not simply by their counts but in a more sophisticated way often boosts the performance. In particular, _TF-IDF_ (term frequency-inverse document frequency) is a popular and effective approach that often leads to better results.\n",
    "\n",
    "Interestingly, there seems to be a German topic (the second one), which should be easier to distinguish from the rest because these words do not occur elsewhere. Three of those five German words, however, are stopwords. Of course, these were not removed since we only removed English stopwords. This highlights the need of multi-language NLP algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fIECxa_7NJs"
   },
   "source": [
    "## B1. Convert tokens into sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_tokens = title_author.flatMapValues(get_tokens).groupByKey()\\\n",
    "                    .map(lambda row: (str(row[0]), row[1])).mapValues(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "c-o5lCix7NJt",
    "outputId": "1260e6ee-fce1-4acb-b55d-5c85e0175f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ignas G. Niemegeers', ['cognitive', 'architecture', 'personal', 'network', 'method', 'analysing', 'performance', 'aspect', 'faulttolerance', 'mechanism', 'fddi', 'density', 'estimation', 'outofrange', 'event', 'personal', 'mobile', 'device', 'cross', 'layer', 'design', 'enhanced', 'quality', 'routing', 'personal', 'wireless', 'networking', 'communication', 'context', 'adaptive', 'mobile', 'application', 'modelling', 'reassembly', 'buffer', 'connectionless', 'server', 'extensible', 'network', 'resource', 'abstraction', 'application', 'mobile', 'device', 'integration', 'umts', 'bisdn', 'system', 'clustering', 'ad', 'hoc', 'personal', 'network', 'formation', 'integration', 'broadband', 'broadcasting', 'wireless', 'technology', 'umts', 'radio', 'access', 'level', 'dvbh', 'umts', 'integration', 'radio', 'access', 'level', 'voice', 'activity', 'detection', 'voipan', 'information', 'theoretic', 'approach', 'ad', 'hoc', 'federation', 'network', 'fednets', 'mechanism', 'requirement', 'system', 'perspective', 'cognition', 'autonomic', 'computing', 'communication', 'research', 'issue', 'qos', 'provisioning', 'personal', 'network', 'mac', 'protocol', 'design', 'impulse', 'radio', 'uwb', 'based', 'wpans', 'privacy', 'anonymity', 'personal', 'network', 'framework', 'integration', 'different', 'wlan', 'technology', 'umts', 'radio', 'access', 'level', 'resource', 'optimization', 'ghz', 'indoor', 'network', 'using', 'dynamic', 'extended', 'cell', 'formation', 'ctit', 'research', 'alliance', 'telematics', 'voice', 'transmission', 'priority', 'csmaac', 'lan', 'efficient', 'protocol', 'using', 'hybrid', 'switching', 'performance', 'modelling', 'hslan', 'slotted', 'ring', 'protocol', 'europcom', 'ultrawideband', 'uwb', 'based', 'ad', 'hoc', 'network', 'emergency', 'application', 'selfconfiguration', 'manet', 'different', 'perspective', 'analyzing', 'effect', 'node', 'mobility', 'clustered', 'wireless', 'ad', 'hoc', 'network', 'variable', 'bandwidth', 'connection', 'connectionless', 'service', 'atmperformance', 'modelling', 'evaluation', 'toward', 'seamless', 'communication', 'architecture', 'inbuilding', 'network', 'ghz', 'band', 'performance', 'analysis', 'link', 'layer', 'protocol', 'uwb', 'impulse', 'radio', 'network', 'solving', 'incertitude', 'vertical', 'handover', 'heterogeneous', 'mobile', 'wireless', 'network', 'using', 'mdp', 'stability', 'ad', 'hoc', 'group', 'mobility', 'model', 'impact', 'umtswcdma', 'channel', 'roundtrip', 'time', 'variation', 'tcp', 'vega', 'performance', 'quantitative', 'evaluation', 'scalability', 'broadband', 'intelligent', 'network', 'performance', 'tcp', 'vega', 'umtswcdma', 'channel', 'large', 'roundtrip', 'time', 'variation', 'selforganizing', 'link', 'layer', 'protocol', 'uwb', 'ad', 'hoc', 'network', 'performability', 'modelling', 'using', 'dynamic', 'queueing', 'network', 'architecture', 'intrapersonal', 'network', 'communication', 'fewpnets', 'framework', 'emulation', 'wireless', 'personal', 'network', 'selforganized', 'personal', 'network', 'architecture', 'experiencebased', 'network', 'resource', 'usage', 'mobile', 'host', 'multihop', 'iruwb', 'wpan', 'architecture', 'protocol', 'biobjective', 'power', 'aware', 'routing', 'algorithm', 'personal', 'network', 'evaluation', 'high', 'speed', 'local', 'area', 'access', 'mechanism', 'analysis', 'ieee', 'radio', 'fiber', 'home', 'network', 'framework', 'selforganization', 'personal', 'network', 'modelling', 'workload', 'multiservice', 'lan'])]\n"
     ]
    }
   ],
   "source": [
    "# example output before removing additonal stopwords\n",
    "print(author_tokens.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['using', 'network', 'model', 'algorithm', 'based', 'system', 'data']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "doc_stop_words = author_tokens.flatMap(lambda r: r[1]).map(lambda r: (r,1)).reduceByKey(lambda a,b: a+b)\n",
    "doc_stop_words = doc_stop_words.filter(lambda a: a[1]>25000).map(lambda r: r[0]).collect()\n",
    "doc_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ignas G. Niemegeers', ['cognitive', 'architecture', 'personal', 'method', 'analysing', 'performance', 'aspect', 'faulttolerance', 'mechanism', 'fddi', 'density', 'estimation', 'outofrange', 'event', 'personal', 'mobile', 'device', 'cross', 'layer', 'design', 'enhanced', 'quality', 'routing', 'personal', 'wireless', 'networking', 'communication', 'context', 'adaptive', 'mobile', 'application', 'modelling', 'reassembly', 'buffer', 'connectionless', 'server', 'extensible', 'resource', 'abstraction', 'application', 'mobile', 'device', 'integration', 'umts', 'bisdn', 'clustering', 'ad', 'hoc', 'personal', 'formation', 'integration', 'broadband', 'broadcasting', 'wireless', 'technology', 'umts', 'radio', 'access', 'level', 'dvbh', 'umts', 'integration', 'radio', 'access', 'level', 'voice', 'activity', 'detection', 'voipan', 'information', 'theoretic', 'approach', 'ad', 'hoc', 'federation', 'fednets', 'mechanism', 'requirement', 'perspective', 'cognition', 'autonomic', 'computing', 'communication', 'research', 'issue', 'qos', 'provisioning', 'personal', 'mac', 'protocol', 'design', 'impulse', 'radio', 'uwb', 'wpans', 'privacy', 'anonymity', 'personal', 'framework', 'integration', 'different', 'wlan', 'technology', 'umts', 'radio', 'access', 'level', 'resource', 'optimization', 'ghz', 'indoor', 'dynamic', 'extended', 'cell', 'formation', 'ctit', 'research', 'alliance', 'telematics', 'voice', 'transmission', 'priority', 'csmaac', 'lan', 'efficient', 'protocol', 'hybrid', 'switching', 'performance', 'modelling', 'hslan', 'slotted', 'ring', 'protocol', 'europcom', 'ultrawideband', 'uwb', 'ad', 'hoc', 'emergency', 'application', 'selfconfiguration', 'manet', 'different', 'perspective', 'analyzing', 'effect', 'node', 'mobility', 'clustered', 'wireless', 'ad', 'hoc', 'variable', 'bandwidth', 'connection', 'connectionless', 'service', 'atmperformance', 'modelling', 'evaluation', 'toward', 'seamless', 'communication', 'architecture', 'inbuilding', 'ghz', 'band', 'performance', 'analysis', 'link', 'layer', 'protocol', 'uwb', 'impulse', 'radio', 'solving', 'incertitude', 'vertical', 'handover', 'heterogeneous', 'mobile', 'wireless', 'mdp', 'stability', 'ad', 'hoc', 'group', 'mobility', 'impact', 'umtswcdma', 'channel', 'roundtrip', 'time', 'variation', 'tcp', 'vega', 'performance', 'quantitative', 'evaluation', 'scalability', 'broadband', 'intelligent', 'performance', 'tcp', 'vega', 'umtswcdma', 'channel', 'large', 'roundtrip', 'time', 'variation', 'selforganizing', 'link', 'layer', 'protocol', 'uwb', 'ad', 'hoc', 'performability', 'modelling', 'dynamic', 'queueing', 'architecture', 'intrapersonal', 'communication', 'fewpnets', 'framework', 'emulation', 'wireless', 'personal', 'selforganized', 'personal', 'architecture', 'experiencebased', 'resource', 'usage', 'mobile', 'host', 'multihop', 'iruwb', 'wpan', 'architecture', 'protocol', 'biobjective', 'power', 'aware', 'routing', 'personal', 'evaluation', 'high', 'speed', 'local', 'area', 'access', 'mechanism', 'analysis', 'ieee', 'radio', 'fiber', 'home', 'framework', 'selforganization', 'personal', 'modelling', 'workload', 'multiservice', 'lan'])]\n"
     ]
    }
   ],
   "source": [
    "author_tokens_no_stop = author_tokens\\\n",
    "    .map(lambda r: (r[0],[w for w in r[1] if not w in doc_stop_words and not len(w)==1]))\n",
    "\n",
    "# show example after removing stopwords\n",
    "print(author_tokens_no_stop.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              author|               words|\n",
      "+--------------------+--------------------+\n",
      "|Christos D. Zarol...|[topic, theory, p...|\n",
      "|   Evaggelia Pitoura|[topic, mobile, u...|\n",
      "|      Valrie Issarny|[topic, mobile, u...|\n",
      "|            Xiao Qin|[dynamic, load, b...|\n",
      "|          Hong Jiang|[dynamic, load, b...|\n",
      "|        Geoffrey Fox|[metacomputing, p...|\n",
      "|       Frdric Vivien|[static, loadbala...|\n",
      "|     Franck Cappello|[private, virtual...|\n",
      "|  Thomas Ludwig 0002|[configurable, lo...|\n",
      "|Koenraad De Bossc...|[detecting, race,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe\n",
    "author_tokens_df = spark.createDataFrame(author_tokens_no_stop, [\"author\",\"words\"])\n",
    "author_tokens_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VkqEpsAHfBC"
   },
   "source": [
    "## Generate a vectorized representation of the *tokens*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              author|               words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "| Ignas G. Niemegeers|[cognitive, archi...|(53195,[0,1,2,3,7...|\n",
      "|     Kees M. van Hee|[policy, iteratio...|(53195,[0,1,2,7,9...|\n",
      "|   Sjaak Brinkkemper|[method, engineer...|(53195,[0,1,2,3,5...|\n",
      "|Alistair G. Sutcl...|[use, domain, kno...|(53195,[0,1,2,3,4...|\n",
      "|   George S. Avrunin|[engineering, med...|(53195,[0,2,3,5,9...|\n",
      "|      Lori A. Clarke|[engineering, med...|(53195,[0,1,2,3,9...|\n",
      "|          Klaus Pohl|[information, qua...|(53195,[0,1,2,3,5...|\n",
      "|       Pedro Antunes|[visuallydriven, ...|(53195,[0,1,2,3,5...|\n",
      "|        Frank Vetere|[decisionmaking, ...|(53195,[3,4,5,8,9...|\n",
      "|      Luciano Baresi|[workflow, partit...|(53195,[0,1,2,3,5...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=2)\n",
    "\n",
    "# fit the vectorizer\n",
    "author_cv_model = cv.fit(author_tokens_df)\n",
    "\n",
    "author_tokens_df_w_features = author_cv_model.transform(author_tokens_df)\n",
    "author_tokens_df_w_features.cache()\n",
    "author_tokens_df_w_features.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdVwBNdt7NJu"
   },
   "source": [
    "## Convert pyspark.ml vectors to pyspark.mllib vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, I perform this conversion but use the feature vectors obtained above for the rest of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(53187, {2: 2.0, 3: 4.0, 4: 2.0, 5: 4.0, 8: 5.0, 9: 3.0, 11: 1.0, 13: 5.0, 14: 2.0, 17: 2.0, 18: 2.0, 19: 4.0, 20: 3.0, 22: 3.0, 26: 4.0, 27: 10.0, 30: 2.0, 31: 1.0, 32: 2.0, 34: 1.0, 36: 3.0, 37: 1.0, 39: 1.0, 41: 1.0, 43: 1.0, 45: 3.0, 46: 1.0, 47: 16.0, 51: 2.0, 53: 3.0, 54: 1.0, 57: 1.0, 58: 1.0, 62: 3.0, 64: 1.0, 65: 1.0, 69: 1.0, 70: 1.0, 71: 2.0, 77: 1.0, 85: 1.0, 87: 2.0, 92: 3.0, 97: 1.0, 98: 2.0, 109: 1.0, 111: 1.0, 115: 1.0, 116: 2.0, 120: 1.0, 124: 1.0, 130: 2.0, 141: 1.0, 143: 1.0, 147: 3.0, 148: 1.0, 156: 1.0, 159: 2.0, 169: 15.0, 179: 2.0, 181: 1.0, 182: 2.0, 184: 4.0, 191: 1.0, 195: 1.0, 197: 4.0, 214: 1.0, 217: 1.0, 222: 6.0, 228: 1.0, 229: 2.0, 233: 1.0, 251: 3.0, 252: 1.0, 255: 5.0, 261: 1.0, 267: 1.0, 269: 2.0, 274: 2.0, 276: 1.0, 283: 1.0, 285: 1.0, 286: 1.0, 290: 2.0, 294: 2.0, 304: 1.0, 305: 1.0, 306: 3.0, 308: 1.0, 309: 4.0, 317: 3.0, 319: 1.0, 324: 1.0, 348: 2.0, 363: 2.0, 374: 1.0, 379: 1.0, 383: 2.0, 385: 1.0, 386: 3.0, 388: 4.0, 406: 1.0, 415: 12.0, 423: 1.0, 432: 9.0, 435: 1.0, 436: 1.0, 437: 1.0, 439: 1.0, 443: 1.0, 445: 5.0, 449: 1.0, 454: 1.0, 471: 3.0, 479: 1.0, 513: 1.0, 530: 1.0, 535: 1.0, 538: 1.0, 540: 8.0, 554: 1.0, 568: 1.0, 586: 1.0, 604: 1.0, 605: 1.0, 622: 1.0, 626: 2.0, 648: 1.0, 655: 1.0, 680: 1.0, 688: 1.0, 706: 1.0, 715: 1.0, 720: 2.0, 793: 1.0, 802: 3.0, 836: 3.0, 840: 1.0, 855: 1.0, 858: 7.0, 862: 1.0, 866: 1.0, 939: 2.0, 949: 14.0, 951: 1.0, 965: 1.0, 971: 1.0, 973: 1.0, 989: 1.0, 997: 1.0, 1008: 1.0, 1048: 1.0, 1056: 1.0, 1066: 1.0, 1081: 1.0, 1086: 1.0, 1089: 6.0, 1096: 1.0, 1099: 1.0, 1116: 2.0, 1119: 3.0, 1156: 4.0, 1175: 1.0, 1177: 1.0, 1221: 1.0, 1224: 1.0, 1272: 1.0, 1278: 1.0, 1335: 1.0, 1350: 1.0, 1365: 4.0, 1375: 14.0, 1378: 1.0, 1413: 1.0, 1428: 1.0, 1450: 1.0, 1451: 1.0, 1477: 1.0, 1489: 3.0, 1494: 1.0, 1517: 1.0, 1519: 2.0, 1670: 1.0, 1731: 1.0, 1732: 1.0, 1778: 3.0, 1793: 1.0, 1798: 1.0, 1799: 3.0, 1815: 1.0, 1879: 1.0, 2129: 1.0, 2166: 3.0, 2193: 1.0, 2207: 2.0, 2263: 1.0, 2305: 3.0, 2311: 2.0, 2344: 1.0, 2353: 3.0, 2437: 1.0, 2600: 1.0, 2639: 1.0, 2700: 1.0, 2784: 1.0, 2962: 1.0, 2984: 1.0, 3119: 1.0, 3290: 1.0, 3419: 1.0, 3444: 1.0, 3559: 1.0, 3621: 1.0, 3635: 1.0, 3653: 1.0, 3837: 1.0, 3845: 1.0, 4076: 1.0, 4184: 1.0, 4276: 1.0, 4282: 1.0, 4435: 1.0, 4437: 1.0, 4633: 1.0, 4962: 1.0, 5382: 1.0, 5421: 1.0, 5549: 1.0, 5769: 1.0, 5876: 1.0, 5970: 1.0, 6228: 2.0, 6564: 1.0, 6935: 1.0, 7047: 1.0, 7473: 1.0, 7504: 1.0, 7866: 1.0, 8087: 1.0, 8290: 1.0, 8576: 1.0, 9003: 1.0, 9578: 1.0, 10225: 1.0, 10843: 1.0, 11058: 1.0, 11061: 1.0, 11891: 1.0, 13338: 1.0, 14601: 1.0, 19696: 1.0, 19771: 1.0, 22227: 1.0, 29239: 2.0, 31246: 1.0, 32713: 1.0, 33429: 1.0, 34365: 1.0, 35020: 1.0, 39274: 1.0, 43390: 1.0, 44250: 1.0, 44555: 1.0, 44822: 1.0})]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def as_mllib_vector(v):\n",
    "    return Vectors.sparse(v.size, v.indices, v.values)\n",
    "\n",
    "features = author_tokens_df_w_features.select(\"features\")\n",
    "feature_vec = features.rdd.map(lambda r: as_mllib_vector(r[0]))\n",
    "\n",
    "# feature_vec.cache()\n",
    "feature_vec.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CatbxUVB7NJv"
   },
   "source": [
    "### Take a look at the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "cgSlC5nT7NJv",
    "outputId": "a269f005-d5c1-4267-ef90-90583853c984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terms W =  53195\n",
      "Vocabulary from CountVectorizerModel is:\n",
      "['analysis', 'application', 'approach', 'design', 'image', 'distributed', 'web', 'service', 'learning', 'information', 'efficient', 'dynamic', 'architecture', 'performance', 'software', 'method', 'database', 'environment', 'parallel', 'control', 'management', 'mobile', 'framework', 'logic', 'query', 'problem', 'detection', 'object', 'modeling', 'video', 'wireless', 'language', 'evaluation', 'adaptive', 'new', 'protocol', 'graph', 'sensor', 'technique', 'process', 'program', 'optimization', 'computing', 'retrieval', 'agent', 'study', 'search', 'programming', 'scheme', 'support', 'scheduling', 'realtime', 'recognition', 'towards', 'semantic', 'communication', 'knowledge', 'mining', 'pattern', 'time', 'simulation', 'implementation', 'multiple', 'structure', 'constraint', 'automatic', 'classification', 'grid', 'routing', 'feature', 'power', 'memory', 'tool', 'generation', 'estimation', 'tree', 'user', 'fast', 'processing', 'virtual', 'robot', 'set', 'clustering', 'development', 'case', 'abstract', 'selection', 'machine', 'access', 'test', 'document', 'security', 'motion', 'space', 'optimal', 'interface', 'circuit', 'multimedia', 'digital', 'resource']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of terms W = \", len(author_cv_model.vocabulary))\n",
    "print (\"Vocabulary from CountVectorizerModel is:\")\n",
    "print(author_cv_model.vocabulary[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWAqVT027NJw"
   },
   "source": [
    "## B1. Latent Dirichlet Allocation\n",
    "\n",
    "We now analyse the same dataset but using the Latent Dirichlet Allocation to find feature vectors characterizing topics of documents, and feature vectors characterizing the words of topics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 ms, sys: 3.12 ms, total: 26.1 ms\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialize and run LDA\n",
    "lda = LDA(k=10, maxIter=5)\n",
    "lda_model = lda.fit(author_tokens_df_w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              author|               words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "| Ignas G. Niemegeers|[cognitive, archi...|(53195,[0,1,2,3,7...|\n",
      "|     Kees M. van Hee|[policy, iteratio...|(53195,[0,1,2,7,9...|\n",
      "|   Sjaak Brinkkemper|[method, engineer...|(53195,[0,1,2,3,5...|\n",
      "|Alistair G. Sutcl...|[use, domain, kno...|(53195,[0,1,2,3,4...|\n",
      "|   George S. Avrunin|[engineering, med...|(53195,[0,2,3,5,9...|\n",
      "|      Lori A. Clarke|[engineering, med...|(53195,[0,1,2,3,9...|\n",
      "|          Klaus Pohl|[information, qua...|(53195,[0,1,2,3,5...|\n",
      "|       Pedro Antunes|[visuallydriven, ...|(53195,[0,1,2,3,5...|\n",
      "|        Frank Vetere|[decisionmaking, ...|(53195,[3,4,5,8,9...|\n",
      "|      Luciano Baresi|[workflow, partit...|(53195,[0,1,2,3,5...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_tokens_df_w_features.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9roli0F7NJs"
   },
   "source": [
    "Looking at the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "z-D8pLq07NJr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|   [6, 3, 1, 10, 16]|[0.00163929522397...|\n",
      "|    1|     [0, 4, 1, 3, 8]|[0.00145573371263...|\n",
      "|    2|  [0, 96, 70, 3, 37]|[0.00279599935651...|\n",
      "|    3|  [70, 2, 0, 13, 74]|[6.09780262412954...|\n",
      "|    4|  [0, 2, 64, 24, 36]|[0.00108986452539...|\n",
      "|    5|    [8, 2, 0, 40, 3]|[8.50447978928385...|\n",
      "|    6|[89, 96, 100, 3, 73]|[0.00539225763913...|\n",
      "|    7|     [0, 2, 1, 4, 5]|[0.00670131264442...|\n",
      "|    8|   [0, 14, 3, 40, 5]|[0.00156100925315...|\n",
      "|    9|   [7, 31, 1, 6, 20]|[3.30012048074474...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "topics = lda_model.describeTopics(5)\n",
    "\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "z-D8pLq07NJr",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['web' 'design' 'application' 'efficient' 'database']\n",
      "['analysis' 'image' 'application' 'design' 'learning']\n",
      "['analysis' 'circuit' 'power' 'design' 'sensor']\n",
      "['power' 'approach' 'analysis' 'performance' 'estimation']\n",
      "['analysis' 'approach' 'constraint' 'query' 'graph']\n",
      "['learning' 'approach' 'analysis' 'program' 'design']\n",
      "['test' 'circuit' 'fault' 'design' 'generation']\n",
      "['analysis' 'approach' 'application' 'image' 'distributed']\n",
      "['analysis' 'software' 'design' 'program' 'distributed']\n",
      "['service' 'language' 'application' 'web' 'management']\n"
     ]
    }
   ],
   "source": [
    "# Shows the results\n",
    "import numpy as np\n",
    "topic_i = topics.select(\"termIndices\").rdd.map(lambda r: r[0]).collect()\n",
    "for i in topic_i:\n",
    "    print(np.array(author_cv_model.vocabulary)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|              author|               words|            features|   topicDistribution|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "| Ignas G. Niemegeers|[cognitive, archi...|(53195,[0,1,2,3,7...|[3.51900543513945...|\n",
      "|     Kees M. van Hee|[policy, iteratio...|(53195,[0,1,2,7,9...|[6.08347798926278...|\n",
      "|   Sjaak Brinkkemper|[method, engineer...|(53195,[0,1,2,3,5...|[2.19178045736207...|\n",
      "|Alistair G. Sutcl...|[use, domain, kno...|(53195,[0,1,2,3,4...|[2.24000989153387...|\n",
      "|   George S. Avrunin|[engineering, med...|(53195,[0,2,3,5,9...|[4.99214653833859...|\n",
      "|      Lori A. Clarke|[engineering, med...|(53195,[0,1,2,3,9...|[2.64666647249385...|\n",
      "|          Klaus Pohl|[information, qua...|(53195,[0,1,2,3,5...|[2.63913948546808...|\n",
      "|       Pedro Antunes|[visuallydriven, ...|(53195,[0,1,2,3,5...|[2.93425266317370...|\n",
      "|        Frank Vetere|[decisionmaking, ...|(53195,[3,4,5,8,9...|[5.88873569226827...|\n",
      "|      Luciano Baresi|[workflow, partit...|(53195,[0,1,2,3,5...|[2.64663231502957...|\n",
      "|        Reiko Heckel|[conceptual, mode...|(53195,[0,1,2,3,6...|[2.00074330264794...|\n",
      "|           Tao Zhang|[resolve, conflic...|(53195,[0,1,2,3,4...|[1.34453791529299...|\n",
      "|             Ping Li|[segmentation, tr...|(53195,[0,1,2,4,8...|[1.87800296708888...|\n",
      "|               Yi Li|[approach, enterp...|(53195,[0,1,2,3,4...|[1.80778424606275...|\n",
      "|          Jing Zhang|[integration, fra...|(53195,[0,1,2,3,4...|[9.11009197281472...|\n",
      "|           Lei Huang|[research, applic...|(53195,[0,1,2,4,5...|[4.13739992632910...|\n",
      "|         Qiang Zhang|[research, geneti...|(53195,[0,2,3,4,5...|[2.85220718932134...|\n",
      "|         Ken Kennedy|[automatic, layou...|(53195,[0,1,5,9,1...|[1.54340031720765...|\n",
      "|         Pter Kacsuk|[dataflow, logicf...|(53195,[0,1,2,3,5...|[2.38542516329234...|\n",
      "|           Evan Tick|[demanddriven, da...|(53195,[0,3,10,12...|[5.13162833087554...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = lda_model.transform(author_tokens_df_w_features)\n",
    "transformed.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AitucvvP7NJw"
   },
   "source": [
    "The perplexity below is a measurement of how well a probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train, test = author_tokens_df_w_features.randomSplit([9.0, 1.0], 446)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def perplexity_vs_doc(lda, train, test, stage):\n",
    "    testPreplexity = np.zeros(stage)\n",
    "    computationTime = np.zeros(stage)\n",
    "    fracs = np.linspace(0.1, 1.0, stage)\n",
    "    num_repeat = 3\n",
    "    for j in range(num_repeat):\n",
    "        for i in range(stage):\n",
    "            corpus = train.sample(False, fracs[i], 10*j)\n",
    "            # Batch variational inference\n",
    "            t0 = time()\n",
    "            lda_model = lda.fit(corpus)\n",
    "            t1 = time()\n",
    "            computationTime[i] += (t1 - t0)/num_repeat\n",
    "            testPreplexity[i] += lda_model.logPerplexity(test)/num_repeat\n",
    "    return testPreplexity, computationTime\n",
    "\n",
    "# set the seeds to have the same inital clustering\n",
    "# batch LDA\n",
    "test_preplexity_em, computation_time_em = perplexity_vs_doc(LDA(k=10, maxIter=5, seed=123), \n",
    "                                                            train, test, 10)\n",
    "# online LDA\n",
    "# test_preplexity_online, computation_time_online = perplexity_vs_doc(LDA(k=10, maxIter=5, optimizer='online', seed=123), \n",
    "#                                                                     train, test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_preplexity_em, label = \"em\")\n",
    "# plt.plot(test_preplexity_online, label = \"online\")\n",
    "plt.xticks(np.arange(0, 10, 1), [str(i+1) for i in np.arange(0, 10, 1)])\n",
    "plt.xlabel(\"training data size *10%\")\n",
    "plt.ylabel(\"testing preplexity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GIlHFm2IM1R"
   },
   "source": [
    "## B2. Calculate the topic density vector for each author and the cosine similarity ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "KhEkCGfm7NJx"
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "def dot_prod(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x.dot(y)\n",
    "\n",
    "# your code\n",
    "def cos_sim(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    \n",
    "    return x.dot(y) / (norm_x*norm_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note**: the implementation of the cosine similarity function is missing the square root over `norm_x`and `norm_y`. The results below are therefore not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              author|   topicDistribution|\n",
      "+--------------------+--------------------+\n",
      "| Ignas G. Niemegeers|[3.51900543513945...|\n",
      "|     Kees M. van Hee|[6.08347798926278...|\n",
      "|   Sjaak Brinkkemper|[2.19178045736207...|\n",
      "|Alistair G. Sutcl...|[2.24000989153387...|\n",
      "|   George S. Avrunin|[4.99214653833859...|\n",
      "|      Lori A. Clarke|[2.64666647249385...|\n",
      "|          Klaus Pohl|[2.63913948546808...|\n",
      "|       Pedro Antunes|[2.93425266317370...|\n",
      "|        Frank Vetere|[5.88873569226827...|\n",
      "|      Luciano Baresi|[2.64663231502957...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------+--------------------+-----------------+--------------------+\n",
      "|             author|   topicDistribution|           author|   topicDistribution|\n",
      "+-------------------+--------------------+-----------------+--------------------+\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|  Kees M. van Hee|[6.08347798926278...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|Sjaak Brinkkemper|[2.19178045736207...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|   Lori A. Clarke|[2.64666647249385...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|       Klaus Pohl|[2.63913948546808...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|    Pedro Antunes|[2.93425266317370...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|   Luciano Baresi|[2.64663231502957...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|     Reiko Heckel|[2.00074330264794...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|        Tao Zhang|[1.34453791529299...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|          Ping Li|[1.87800296708888...|\n",
      "|Ignas G. Niemegeers|[3.51900543513945...|            Yi Li|[1.80778424606275...|\n",
      "+-------------------+--------------------+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors_dist = transformed.select(['author', 'topicDistribution'])\n",
    "authors_dist.show(10)\n",
    "\n",
    "# create a temporary view for SQL queries\n",
    "authors_dist.createOrReplaceTempView(\"authors_dist\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM authors_dist a1\n",
    "    CROSS JOIN authors_dist a2\n",
    "    WHERE a1.author < a2.author\n",
    "    \"\"\"\n",
    "\n",
    "authors_pairs = spark.sql(query)\n",
    "authors_pairs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcMjLVY1JXNG"
   },
   "source": [
    "## B3. Show the 10 most similar author pairs and comment on their similarity,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ignas G. Niemegeers', 'Kees M. van Hee', 0.9999997013727754),\n",
       " ('Ignas G. Niemegeers', 'Sjaak Brinkkemper', 0.999999920568635),\n",
       " ('Ignas G. Niemegeers', 'Lori A. Clarke', 0.9999999656581098),\n",
       " ('Ignas G. Niemegeers', 'Klaus Pohl', 0.9999999650636611),\n",
       " ('Ignas G. Niemegeers', 'Pedro Antunes', 0.9999999845596361),\n",
       " ('Ignas G. Niemegeers', 'Luciano Baresi', 0.9999999656543556),\n",
       " ('Ignas G. Niemegeers', 'Reiko Heckel', 0.9999998960932283),\n",
       " ('Ignas G. Niemegeers', 'Tao Zhang', 0.9999997871165618),\n",
       " ('Ignas G. Niemegeers', 'Ping Li', 0.9999998786411788),\n",
       " ('Ignas G. Niemegeers', 'Yi Li', 0.9999998680476055)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_dist_rdd = authors_pairs.rdd.map(lambda row: (row[0], row[2], cos_sim(row[1], row[3]))) # .cache()\n",
    "\n",
    "# not sorted\n",
    "author_dist_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 12.6 ms, total: 130 ms\n",
      "Wall time: 12min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Angelo Montanari', 'Jiang Li', 1.0000000000000002),\n",
       " ('Andreas Podelski', 'Jianhua Ma', 1.0000000000000002),\n",
       " ('Andreas Podelski', 'David Abramson', 1.0000000000000002),\n",
       " ('Kjetil Nrvg', 'Xi Li', 1.0000000000000002),\n",
       " ('Daniel Schwabe', 'Janne Heikkil', 1.0000000000000002),\n",
       " ('Omran A. Bukhres', 'William E. Weihl', 1.0000000000000002),\n",
       " ('Ramesh Govindan', 'Seog Park', 1.0000000000000002),\n",
       " ('Ernst-Rdiger Olderog', 'V. Richard Benjamins', 1.0000000000000002),\n",
       " ('Linpeng Huang', 'Ralf Schenkel', 1.0000000000000002),\n",
       " ('Allison Woodruff', 'Rahul Garg', 1.0000000000000002)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# sorted\n",
    "author_dist_rdd.takeOrdered(10, key = lambda r: -r[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most similar authors have a cosine similarity that is larger than 1. As already mentioned, this is due to the incorrect implementation of the cosine similarity function. Actually, even in this case it should not be possible but it is most likely a rounding error that occurs due to the distributed computations that Spark performs. Most authors have a very high cosine similarity, most likely due to the mis-specified function.\n",
    "\n",
    "Generally speaking, it can be expected that the most similar authors publish in the same field (e.g. mechanical engineering or computer science). The reason is quite obvious: those authors use a similar vocabulary which may be substantially different from authors in other fields.\n",
    "\n",
    "In addition, it can be expected that authors who have common publications have a particularly high cosine similarity. This is true not only for pairs of authors with a large number of joint publications but also (perhaps in particular) for pairs of authors with only few publications but of which they have published all or most jointly. In the extreme case, two authors who have only one joint publication and no others would have a cosine similarity of 1. To check whether this is indeed the case for some of the authors listed above, one would have to perform the query that was part of assignment 1."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw_dblp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
