---
title: "P4 - Optimization Questions"
subtitle: "ST446 Distributed Computing for Big Data"
author: "Andreas StÃ¶ffelbauer"
date: "5 April 2021"
output: pdf_document
---

## P4-i) Convex Optimization Problem

$$
f(x) = c+g^Tx+\frac12x^THx
$$

The gradient and the Hessian are

$$
\nabla f(x) = g+Hx
$$

$$
\nabla^2f(x) = H
$$

Function $f(x)$ is convex if and only if either (all are equivalent) the Hessian is positive semi-definite, or $xHx\ge0$ for all $x$, or all eigenvalues of the Hessian are non-negative. This is a sufficient condition.

A necessary but not sufficient condition is that all $a_1,...,a_{n-2}$ and $tr(A)=(a_{n-1}, a_n)$ are non-negative. In fact, $a_1,...,a_{n-2}$ are the eigenvalues of $D$ (see part ii).

Note that $xHx$ using only the first $n-2$ dimensions leads to $a_1x_1^2+a_2x_2^2+...+a_{n-2}x_{n-2}^2$ which is non-negative if $a_1,...,a_{n-2}\ge0$. So what remains is the condition on $B$. In fact, another necessary condition for this is that all pivots of $B$ must be non-negative whereas the sufficient condition is that the eigenvalues of $B$ must be non-negative.

## P4-ii) Gradient Descent Algorithm

A (local) minimum of $f(x)$ can be found by repeatedly applying the following gradient descent update using the first derivative $\nabla f(x) = g+Hx$ and a step size parameter $\eta$.

$$
x^{t+1}= x^t - \eta\nabla f(x^t) = x^t - \eta(g+Hx^t)
$$

## P4- iii) $\beta$-Smoothness

Definition of $\beta$-smoothness: $\nabla^2 f(x)\preccurlyeq\beta I$.

Since $\nabla^2 f(x) = H$, function $f(x)$ is $\beta$-smooth if and only if $H\preccurlyeq\beta I$. That is, if and only if all the eigenvalues of the Hessian of $f(x)$ are at most equal to $\beta$.

The eigenvalues of the Hessian can be calculated by solving $$
0=det(H-\lambda I_n) = det(D-\lambda I_{n-2})\,det(B-\lambda I_2)
$$This is due to the fact that $H$ is a diagonal block matrix. We can solve this by independently finding the eigenvalues of $D$ and $B$.

The eigenvalues of a diagonal matrix are its diagonal elements. Therefore, the eigenvalues of matrix $D$ are simply $a_1, a_2,...,a_{n-2}$.

The eigenvalues of of $B$ can be found by solving

$$
0=det(B-\lambda I) = det
\begin{pmatrix}
a_{n1}-\lambda & b\\
b & a_n-\lambda
\end{pmatrix}
= (a_{n-1}-\lambda)(a_n-\lambda) - b^2
$$

which is a quadratic equation with two solutions $\lambda_{n-1}$ and $\lambda_n$ (not necessarily unique). Since $B$ is symmetric, we know that these eigenvalues indeed exist (that is, they are real).

By definition, function $f(x)$ is $\beta$-smooth with

$$
\beta \le max\{a_1,...,a_{n-2},\lambda_{n-1}, \lambda_n\}
$$

## P4- iv) Upper Bound

The obvious eigenvalues of $H$ are $\lambda_1=6, \lambda_2=4$. In addition, we need the two eigenvalues of the matrix $B$ at the center of $H$. Those eigenvalues can be calculated by solving

$$
0=det(B-\lambda I) =det
\begin{pmatrix}
4-\lambda & 2\\
2 & 1-\lambda
\end{pmatrix}
$$

That is, $0 = det(B-\lambda I) = (4-\lambda)(1-\lambda) - 2^2 = \lambda^2-5\lambda = \lambda(\lambda-5)$.

This leads to the eigenvalues $\lambda_3 = 0$ and $\lambda_4 = 5$.

An upper bound on the difference of $f(x^T)$ and $f(x^*)$ based on the $\beta$-smoothness condition of $f(x)$ using step size $\eta=\frac1\beta = \frac16$ satisfies

$$
f(x^T)-f(x^*) \le 2\beta ||x^0-x^*||^2 \frac1{T-1} = 12 ||x^0-x^*||^2 \frac1{T-1}
$$

which decreases with a factor $\frac1{T-1}$.

In addition, we can also determine $\alpha$-strong convexity.

Definition:$\nabla^2 f(x)\succcurlyeq\alpha I$, or in words: $f(x)$ is $\alpha$-strongly convex if and only if the smallest eigenvalue of $f(x)$ is greater or equal to $\alpha$.

We can see that $f(x)$ is not strongly convex since strong convexity requires $\alpha>0$ and $\min_i\{\lambda_i\}=0$.

An upper bound on the difference of $f(x^T)$ and $f(x^*)$ based on both $\beta$-smoothness and $\alpha$-strong convexity with step size $\eta=\frac2{\alpha+\beta}=\frac26$ would be

$$
f(x^T)-f(x^*) \le \frac\beta2 ||x^0-x^*||^2\exp(-4\frac\alpha{\alpha+\beta}(T-1)) = 3||x^0-x^*||^2
$$

which is independent of $T$ and therefore suggests that the gradient descent will never converge. In fact, $f(x)$ is only convex and not strictly convex since the Hessian is only positive semi-definite and not positive definite. That means, the gradient descent can get stuck in or converge to a saddle point.

## P4- v) Upper Bound \#2

The eigenvalues of $H$ are $\lambda_1=6, \lambda_2=4$ plus the two eigenvalues of the matrix $B$, which can be obtained as follows.

$0=det(A-\lambda I) = (4-\lambda)(4-\lambda) - 2^2 = \lambda^2-8\lambda+12$

Solving this equation yields $\lambda_3=2$ and $\lambda_4=6$.

We can now see that $f(x)$ is both $\beta$-smooth with $\beta=\max_i\{\lambda_i\}=6$ and $\alpha$-strongly convex with $\alpha=\min_i\{\lambda_i\}=2$.

An upper bound on the difference of $f(x^T)$ and $f(x^*)$ using only the $\beta$-smoothness condition and a step size of $\eta=\frac1\beta=\frac16$ is

$$
f(x^T)-f(x^*) \le 2\beta ||x^0-x^*||^2 \frac1{T-1} = 12||x^0-x^*||^2 \frac1{T-1}
$$

Using not only the $\beta$-smoothness but also the $\alpha$-strongly convex condition, an upper bound using step size $\eta=\frac2{\alpha+\beta}=\frac14$ is

$$
f(x^T)-f(x^*) \le \frac\beta2 ||x^0-x^*||^2\exp(-4\frac\alpha{\alpha+\beta}(T-1)) = 3||x^0-x^*||^2\exp(-T+1)
$$

This upper bound converges to zero exponentially with a factor of $\exp(-T+1)$ whereas the upper bound based on smoothness decreases more slowly with a factor of $\frac1{T-1}$.
