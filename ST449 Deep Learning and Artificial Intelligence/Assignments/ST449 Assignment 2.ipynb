{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "silent-hollywood",
   "metadata": {},
   "source": [
    "# Assignment 2 - ST449 Deep Learning and Artificial Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "missing-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from itertools import product\n",
    "from environments import *\n",
    "\n",
    "# set numpy pring options\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-replication",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "### Question 1.1\n",
    "\n",
    "I provide two solutions to this question. One is based on reducing the problem to a Markov Reward Process (a) and the other one is based on iterative policy evaluation (b).\n",
    "\n",
    "*Note: The multi-line equations do not render properly in GitHub, please read the HTML file for the correct solution. This is only relevant for this part.*\n",
    "\n",
    "#### a) Solution based on reduction to a Markov Reward Process (MRP)\n",
    "\n",
    "Given complete knowledge about the dynamics and rewards of the environment, a finite Markov Reward Process (MRP) can be solved in closed form by reforming the Bellman equation for an MRP.\n",
    "\n",
    "$$\n",
    "V = R+\\gamma PV\\\\\\\\\n",
    "V-\\gamma PV = R\\\\\\\\\n",
    "(I-\\gamma P)V = R\\\\\\\\\n",
    "V = (I-\\gamma P)^{-1}R\n",
    "$$\n",
    "\n",
    "An MRP is defined by the tuple $(S,P,R,\\gamma)$ consisting of the set of states $S$, the transition matrix $P$, the reward function $R$, and a discount factor $\\gamma$. There are no actions in an MRP. An MDP, by contrast, is defined by $(S,P,R,A,\\gamma)$, where the additional $A$ represents a set of actions. \n",
    "\n",
    "An important fact is that **given a policy, an MDP reduces to an MRP**. In other words, we can represent an MDP as an MRP with the tuple $(S, R^{\\pi}, P^{\\pi}, \\gamma)$. As a result, the Markov Decision Process (MDP) can be solved by reduction to an MRP. That is, $V^{\\pi}$ can be calculated in closed form as follows.\n",
    "\n",
    "$$\n",
    "V^{\\pi} = R^{\\pi}+\\gamma P^{\\pi}V^{\\pi}\\\\\\\\\n",
    "...\\\\\\\\\n",
    "V^{\\pi} = (I-\\gamma P^{\\pi})^{-1}R^{\\pi}\n",
    "$$\n",
    "\n",
    "where $R^{\\pi}=R^{\\pi}(s) = \\sum_a\\pi(a|s)R(s,a)$ is a vector of expected immediate rewards under policy $\\pi$ and $P^{\\pi}=P^{\\pi}(s'|s) = \\sum_a\\pi(a|s)P(s'|s,a)$ is the matrix of transition probabilities under policy $\\pi$. By defining $R^{\\pi}$ and $P^{\\pi}$, we reduce the MDP to an MRP, which can be solved analytically.\n",
    "\n",
    "Note that $V^{\\pi} = R^{\\pi}+\\gamma P^{\\pi}V^{\\pi}$ is the Bellman equation in matrix notation, which is equivalent to $V_\\pi(s)=\\sum_a\\pi(a|s)\\sum_{s',r} p(s'r|s,a)(r+\\gamma V_\\pi(s')$ for all $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-communist",
   "metadata": {},
   "source": [
    "##### Deriving all policies\n",
    "The goal of this exercise is to evaluate all deterministic policies. In a finite MDP, there are $|A|^{|S|}$ unique deterministic policies, which means that in this problem we have $2^5=32$ policies.\n",
    "\n",
    "I represent the as vectors of length 7, including the two terminal states. Action $LOW$ is represented by 0 and $HIGH$ by 1. Note that the action in the terminal states does not matter because the agent does not get to act once she reaches that state. Therefore, most often I will only show the five non-terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "musical-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of polices: 32\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 1 0 1]\n",
      " [0 0 1 1 0]\n",
      " [0 0 1 1 1]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 1]\n",
      " [0 1 0 1 0]\n",
      " [0 1 0 1 1]\n",
      " [0 1 1 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 1 1 1 0]\n",
      " [0 1 1 1 1]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 1]\n",
      " [1 0 0 1 0]\n",
      " [1 0 0 1 1]\n",
      " [1 0 1 0 0]\n",
      " [1 0 1 0 1]\n",
      " [1 0 1 1 0]\n",
      " [1 0 1 1 1]\n",
      " [1 1 0 0 0]\n",
      " [1 1 0 0 1]\n",
      " [1 1 0 1 0]\n",
      " [1 1 0 1 1]\n",
      " [1 1 1 0 0]\n",
      " [1 1 1 0 1]\n",
      " [1 1 1 1 0]\n",
      " [1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "HIGH = 1\n",
    "LOW = 0\n",
    "\n",
    "# list all polices\n",
    "policies = [list(i) for i in product(range(2), repeat=5)]\n",
    "policies = np.array([[0] + policy + [0] for policy in policies])\n",
    "print('# of polices:', len(policies))\n",
    "print(policies[:, 1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-jefferson",
   "metadata": {},
   "source": [
    "In addition to the 32 policies, this solution also requires all 32 transition matrices $P^{\\pi}$ and reward vectors $R^{\\pi}$. Both can be easily inferred from the description of the game (or the environment essentially), although it is not that straightforward to construct them.\n",
    "\n",
    "Importantly, I compute the transition matrices and reward vectors not only because I want to present my closed form MRP solution here. As will become clear later, the iterative policy evaluation algorithm can also be elegantly implemented in a vectorized fashion using these transition matrices and reward vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "northern-permit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Transition matrix for policy [0 0 0 0 0 0 0], or 'always choose LOW':\n",
      "\n",
      "[[0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.55 0.   0.45 0.   0.   0.   0.  ]\n",
      " [0.   0.55 0.   0.45 0.   0.   0.  ]\n",
      " [0.   0.   0.55 0.   0.45 0.   0.  ]\n",
      " [0.   0.   0.   0.55 0.   0.45 0.  ]\n",
      " [0.   0.   0.   0.   0.55 0.   0.45]\n",
      " [0.   0.   0.   0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "transition_matrices = []\n",
    "\n",
    "Tsa = np.zeros((2,7,7))\n",
    "\n",
    "for i in range(1,6):\n",
    "    Tsa[0, i, i-1] = 0.55\n",
    "    Tsa[0, i, i+1] = 0.45\n",
    "    \n",
    "for i in range(1,6):\n",
    "    Tsa[1, i, i-1] = 0.45\n",
    "    Tsa[1, i, i+1] = 0.55\n",
    "\n",
    "# generate the transition matrix for each policy\n",
    "for policy in policies:\n",
    "    P = Tsa[policy, range(7)]\n",
    "    transition_matrices.append(P)\n",
    "\n",
    "print(f\"Example: Transition matrix for policy {policies[0]}, or 'always choose LOW':\\n\")\n",
    "print(transition_matrices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "structured-malta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Vector of expected immediate rewards for policy 'always choose LOW':\n",
      "[  0. -10. -10. -10. -10. 440.   0.]\n"
     ]
    }
   ],
   "source": [
    "# generate the vector of expected immediate rewards for each policy\n",
    "reward_vectors = []\n",
    "\n",
    "Rsa = np.zeros((7,2))\n",
    "\n",
    "for i in range(1,6):\n",
    "    Rsa[1:6, 0] = -10\n",
    "    Rsa[1:6, 1] = -50\n",
    "\n",
    "    Rsa[5, 0] = -10*0.55 + 0.45*990\n",
    "    Rsa[5, 1] = -50*0.45 + 0.55*950\n",
    "\n",
    "for policy in policies:\n",
    "    reward_vectors.append(Rsa[range(7), policy])\n",
    "    \n",
    "print(\"Example: Vector of expected immediate rewards for policy 'always choose LOW':\")\n",
    "print(reward_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-confirmation",
   "metadata": {},
   "source": [
    "In the code cell below, I show that a closed form solution for a policy (for example \"always choose LOW\") can indeed be obtained by calculating\n",
    "\n",
    "$$\n",
    "V^{\\pi} = (I-\\gamma P^{\\pi})^{-1}R^{\\pi}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "desirable-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state values: [  0.    52.37 138.6  266.21 444.41 684.42   0.  ]\n"
     ]
    }
   ],
   "source": [
    "# solve MDP in closed form\n",
    "P = transition_matrices[0]\n",
    "R = reward_vectors[0]\n",
    "\n",
    "V = np.linalg.inv(np.eye(7)-P).dot(R)\n",
    "print('state values:', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-court",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### b) Iterative Policy Evaluation\n",
    "\n",
    "Solving an MRP involves computing a matrix inverse, which has computational complexity $O(N^3)$ and is therefore unfeasible if the state and/or action space are large. In that case, iterative policy evaluation can be used.\n",
    "\n",
    "As already mentioned, the transition matrices and reward vectors were not only useful for obtaining a closed form solution, they also vastly simplify the implementation of the _iterative policy evaluation_ algorithm. That is, instead of for loops, plain and elegant matrix algebra can be used to update the state values. More specifically, $V^{\\pi}$ can be obtained by repeatedly applying the Bellman backup to some initial $V^{0}$ until a convergence criterion is met.\n",
    "\n",
    "In mathematical terms, the update can be written as follows.\n",
    "\n",
    "$$\n",
    "V_{\\pi}^{t+1} = R^{\\pi} + P^{\\pi}V_{\\pi}^{t} = BV_{\\pi}^{t}\n",
    "$$\n",
    "\n",
    "where $B$ represents the **Bellman backup** (and not a matrix multiplication). Applying this backup repeatedly can be seen as \n",
    "\n",
    "$$\n",
    "V_{\\pi} = BB...BV_{\\pi}^{0}\n",
    "$$\n",
    "\n",
    "Even though the matrix algebra makes this solution more elegant compared to a solution using for loops, there is a potential downside: one Bellman backup in matrix form essentially represents a full sweep over all states. In contrast, by using a for loop, the state value updates can be done in-place, which may speed up convergence.\n",
    "\n",
    "Below is an estimate for the state values of the policy \"always choose LOW\". As can be seen, the state values are equivalent to the ones obtained in closed form above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vocal-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state values: [  0.    52.37 138.6  266.21 444.41 684.42   0.  ]\n"
     ]
    }
   ],
   "source": [
    "V = np.zeros(7)\n",
    "\n",
    "deltaV = 1\n",
    "# repeatedly apply bellman update\n",
    "while deltaV >= 1e-6:\n",
    "    V_old = V\n",
    "    V = R + P.dot(V)\n",
    "    deltaV = np.linalg.norm(V-V_old)\n",
    "\n",
    "print('state values:', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-cornell",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### Estimate all 32 policies\n",
    "To get state value estimates for all 32 policies, we can apply this iterative policy evaluation algorithm to each policy $\\pi$. Finally, we sort the policies by their value in the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "social-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the state values for each policy\n",
    "state_values = []\n",
    "\n",
    "# run policy evlauation for each policy\n",
    "for pi, T, R in zip(policies, transition_matrices, reward_vectors):\n",
    "    \n",
    "    V = np.zeros(7)\n",
    "\n",
    "    deltaV = 1\n",
    "    # repeatedly apply bellman update\n",
    "    while deltaV >= 1e-6:\n",
    "        V_old = V\n",
    "        V = R + T.dot(V)\n",
    "        deltaV = np.linalg.norm(V-V_old)\n",
    "        \n",
    "    state_values.append(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "saving-management",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal policy: [0 0 0 1 1]\n",
      "state values: [  0.    56.52 147.83 281.65 467.43 710.35   0.  ]\n"
     ]
    }
   ],
   "source": [
    "# get the max of state 3 (start state)\n",
    "best_policy_index = np.array([x[3] for x in state_values]).argmax()\n",
    "\n",
    "# best policy is LOW whenever d<=0 and HIGH for d>0\n",
    "print('optimal policy:', policies[best_policy_index, 1:6])\n",
    "print('state values:', state_values[best_policy_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-desktop",
   "metadata": {},
   "source": [
    "##### Discussion of the Results\n",
    "The optimal deterministic policy for starting at the initial state chooses action $HIGH$ whenever the player has a lead over her opponent, and $LOW$ otherwise. \n",
    "\n",
    "Most likely, this is because the agent is driven by the positive reward in the states close the winning state while in states that are closer to loosing, the agent does not want to occur the high cost of action $HIGH$ when the likelihood of losing is quite high in any way. Generally, among the top 10 policies, most confirm this pattern of choosing high in states close to a win and low in states closer to a loss.\n",
    "\n",
    "In fact, this policy is not only the optimal fixed policy when starting at the initial state but also the optimal policy overall. To recap, an optimal policy $\\pi^*$ is defined as a policy whose state values are greater than or equal to those of all other policies. This is the case for policy $[0,0,0,1,1]$.\n",
    "\n",
    "In addition, it is also worth mentioning that an optimal policy is always a deterministic policy (that is, unless there are states in which both actions are equally good (have the same action values), in which case any policy that stochastically chooses among them is optimal too). This is due to the fact that the optimal action value function $Q^*$ is unique (it is the unique solution to the Bellman equation). As a result of our exhaustive search for the best (deterministic) policy, we can be sure that $[0,0,0,1,1]$ is indeed the optimal policy for that game. In particular, no stochastic policy can be as good (and we already know that all deterministic policies are worse).\n",
    "\n",
    "Below is the ranking of all 32 polices from best to worst (based on the initial state). Intriguingly, the two best polices are the opposites of the two worst policies, but this pattern is not consistent throughout the best/worst 16 policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wound-threat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies ranked from best to worst:\n",
      "[[0 0 0 1 1]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 1 1]\n",
      " [0 0 1 0 1]\n",
      " [0 0 1 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 1 1]\n",
      " [0 1 0 0 1]\n",
      " [0 1 1 1 1]\n",
      " [0 1 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 1 1 1 0]\n",
      " [0 1 1 0 0]\n",
      " [1 0 0 1 1]\n",
      " [1 0 0 0 1]\n",
      " [1 0 0 1 0]\n",
      " [1 0 1 1 1]\n",
      " [1 0 0 0 0]\n",
      " [1 0 1 0 1]\n",
      " [1 0 1 1 0]\n",
      " [1 0 1 0 0]\n",
      " [1 1 0 1 1]\n",
      " [1 1 1 1 1]\n",
      " [1 1 0 0 1]\n",
      " [1 1 0 1 0]\n",
      " [1 1 1 0 1]\n",
      " [1 1 0 0 0]\n",
      " [1 1 1 1 0]\n",
      " [1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# sort and print all polcies\n",
    "np.set_printoptions(suppress=True)\n",
    "state_values = np.array(state_values)\n",
    "ranking = reversed(np.argsort(state_values[:, 1]).tolist())\n",
    "print(\"Policies ranked from best to worst:\")\n",
    "print(policies[list(ranking), 1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-acrylic",
   "metadata": {},
   "source": [
    "### Question 1.2 - Off-policy Monte Carlo Prediction (Policy Evaluation)\n",
    "From the instructions, it is not quite clear whether the evaluation policy should be $[1,0,0,0,0]$ or $[1,1,1,1,0]$, depending on which encoding of the states is implied. I went with the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "provincial-intensity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation policy: [1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# define the two policies\n",
    "def evaluation_policy(state):\n",
    "    if state <= 4:\n",
    "        return HIGH\n",
    "    else:\n",
    "        return LOW\n",
    "    \n",
    "def behavior_policy(state):\n",
    "    return np.random.choice(2)\n",
    "\n",
    "# show eval policy,\n",
    "print('Evaluation policy:', [evaluation_policy(state) for state in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "refined-tower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 500000/500000 [00:30<00:00, 16397.04it/s]\n"
     ]
    }
   ],
   "source": [
    "env = Game()\n",
    "\n",
    "# intialize\n",
    "Q = np.zeros((7,2))\n",
    "C = np.zeros((7,2))\n",
    "gamma = 1\n",
    "\n",
    "episodes = 500000\n",
    "for episode in tqdm(range(episodes)):\n",
    "    \n",
    "    history = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    # generate an episode\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        action = behavior_policy(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        history.append((state, action, reward))\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    G = 0\n",
    "    W = 1\n",
    "\n",
    "    # do the MC updates\n",
    "    for state, action, reward in reversed(history):\n",
    "    \n",
    "        G = gamma*G + reward\n",
    "        C[state, action] += W\n",
    "        Q[state, action] += (W/C[state, action]) * (G-Q[state, action])\n",
    "                \n",
    "        if evaluation_policy(state) == action:\n",
    "            W *= 1/0.5\n",
    "        else:\n",
    "            W = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "temporal-currency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action values:\n",
      "[[  0.     0.  ]\n",
      " [ 56.8   18.9 ]\n",
      " [137.7  147.29]\n",
      " [327.86 339.01]\n",
      " [498.38 574.88]\n",
      " [806.3  718.13]\n",
      " [  0.     0.  ]]\n",
      "suggested policy: [0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "## MC prediction results, 500K episodes\n",
    "print('action values:')\n",
    "print(Q)\n",
    "print('suggested policy:', np.argmax(Q, axis=1)[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exempt-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action values:\n",
      "[[  0.     0.  ]\n",
      " [ 54.96   1.13]\n",
      " [111.17  62.79]\n",
      " [298.94 159.11]\n",
      " [426.89 448.48]\n",
      " [658.42 676.95]\n",
      " [  0.     0.  ]]\n",
      "suggested policy: [0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "## MC prediction results, 500K episodes\n",
    "print('action values:')\n",
    "print(Q)\n",
    "print('suggested policy:', np.argmax(Q, axis=1)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-polls",
   "metadata": {},
   "source": [
    "To begin with, the Monte Carlo results were very unstable, which is why it is difficult to interpret this very result. Above are two runs of 500K episodes. The action values are more different that expected because in theory, Monte Carlo estimates are guaranteed to converge to the true values (see 1.3 for further details about that).\n",
    "\n",
    "Generally speaking, because the policy inferred from the action values is different from the evaluation policy, we can say that it is not the optimal policy (assuming that the action values indeed converged).\n",
    "\n",
    "In other words, the action values suggest a policy that is different from the evaluation policy. By the _policy improvement theorem_, if we act greedily with respect to these action values, we can improve upon the original policy, i.e. the evaluation policy. This is because\n",
    "\n",
    "$$\n",
    "q_\\pi(s, \\pi'(s)) \\ge v_\\pi(s)\n",
    "$$\n",
    "\n",
    "where $\\pi' = \\arg\\max_a q_\\pi(s,a)$ is always true. This is the general idea of _policy iteration_.\n",
    "\n",
    "Indeed, when compared to the ranking of policies from part 1.1, we can see that the newly suggested policies (in both runs) are indeed better than the evaluation policy (which is the second worst deterministic policy). The second run actually suggests the optimal policy. However, as already mentioned, this is not always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-space",
   "metadata": {},
   "source": [
    "### Question 1.3 - Off-policy Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "generous-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(qvalues):\n",
    "    return np.argmax(qvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "comparative-animation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 500000/500000 [00:34<00:00, 14368.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize the environment\n",
    "env = Game()\n",
    "\n",
    "# intialize\n",
    "Q = np.zeros((7,2))\n",
    "C = np.zeros((7,2))\n",
    "gamma = 1\n",
    "\n",
    "episodes = 500000\n",
    "for episode in tqdm(range(episodes)):\n",
    "          \n",
    "    history = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    # generate an episode\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        action = behavior_policy(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        history.append((state, action, reward))\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "    G = 0\n",
    "    W = 1\n",
    "        \n",
    "    # perform the MC updates\n",
    "    for state, action, reward in reversed(history):\n",
    "        \n",
    "        G = gamma*G + reward\n",
    "        C[state, action] += W\n",
    "        Q[state, action] += (W/C[state, action]) * (G-Q[state, action])\n",
    "        \n",
    "        max_action = greedy_policy(Q[state])\n",
    "                \n",
    "        if action != max_action:\n",
    "            break\n",
    "        else:\n",
    "            W /= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "velvet-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.     0.  ]\n",
      " [  5.56  18.46]\n",
      " [129.66 155.63]\n",
      " [358.16 364.81]\n",
      " [580.54 547.37]\n",
      " [787.41 795.  ]\n",
      " [  0.     0.  ]]\n",
      "suggested optimal policy: [1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "## MC control results, 500K episodes\n",
    "print(Q)\n",
    "print('suggested optimal policy:', np.argmax(Q, axis=-1)[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "altered-spotlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.     0.  ]\n",
      " [ 13.94  28.64]\n",
      " [199.6  134.24]\n",
      " [411.6  317.52]\n",
      " [524.96 520.96]\n",
      " [690.15 750.18]\n",
      " [  0.     0.  ]]\n",
      "suggested optimal policy: [1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "## MC control results, 500K episodes\n",
    "print(Q)\n",
    "print('suggested optimal policy:', np.argmax(Q, axis=-1)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-decade",
   "metadata": {},
   "source": [
    "As before, the action value estimates are quite unstable and the resulting optimal policy changes accordingly from run to run. Generally, we would expect the algorithm to find the optimal policy from part 1.1, which is $[0,0,0,1,1]$.\n",
    "\n",
    "A potential reason could be that the number of episodes is too low. In fact, off-policy Monto Carlo is very inefficient using a random behavior policy. The probability for only one single action value update is $0.5$. The probability of two updates is $0.5^2=0.25$, for three it is $0.5^3=0.125$, and so on. Note that the updates occur in reversed order, so only the last (few) state-action values receive updates (if any). Therefore, even though some episodes last more than ten rounds, the number of updates is typically much lower. \n",
    "\n",
    "To highlight this problem: say the agent reaches state 5 (just before winning the game) but ends up in state 0 (the losing state). In this this episode, state 5 will most likely not receive an update, but only the state-action values close to 0. Of course, this problem also applies to the Monte Carlo policy evaluation (part 1.2) and may also explain the instability of the action value estimates there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-consciousness",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "### Question 2.1 - SARSA\n",
    "In this and the next part (Q-learning), I use a step-size of $\\alpha=0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "compound-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the behavior policy\n",
    "def eps_greedy_policy(qsa, epsilon=0.1):\n",
    "    if np.random.binomial(1, epsilon) == 1:\n",
    "        return np.random.choice(2) # only two actions\n",
    "    else:\n",
    "        return np.random.choice([action_ for action_, value_ in enumerate(qsa) if value_ == np.max(qsa)])\n",
    "\n",
    "# sarsa update function\n",
    "def sarsa(qsa, next_qsa, r, alpha=0.001, gamma=1.0):\n",
    "    return qsa + alpha * (r + gamma * next_qsa - qsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "plastic-recovery",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500000/500000 [02:10<00:00, 3842.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize action values\n",
    "Q = np.zeros((7,2))\n",
    "\n",
    "episodes = 500000\n",
    "for episode in tqdm(range(episodes)):\n",
    "        \n",
    "    # reset the env and get fist state\n",
    "    state = env.reset()\n",
    "    \n",
    "    # for sarsa, first action before loop\n",
    "    action = eps_greedy_policy(Q[state])\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # step environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # get sarsa next action, on-policy\n",
    "        next_action = eps_greedy_policy(Q[next_state])\n",
    "        \n",
    "        # sarsa update\n",
    "        Q[state, action] = sarsa(Q[state, action], Q[next_state, next_action], reward)\n",
    "        \n",
    "        # update state and action\n",
    "        state = next_state\n",
    "        action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "environmental-fitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.        ]\n",
      " [ 55.33392574  30.51102454]\n",
      " [141.55013044 127.78302871]\n",
      " [281.33146712 267.08369879]\n",
      " [453.51996653 471.27609031]\n",
      " [695.67626223 720.29911683]\n",
      " [  0.           0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SARSA results 500K episodes, alpha 0.001\n",
    "print(Q)\n",
    "np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-growth",
   "metadata": {},
   "source": [
    "The Q-values learned by SARSA suggest a policy that chooses action $HIGH$ whenever she has a lead over her opponent, otherwise $LOW$. This coincides with the optimal policy found through exhaustive search. \n",
    "\n",
    "It is not always the case that SARSA indeed finds the optimal policy as it essentially solves a slightly different problem: it searches not for the \"globally\" optimal policy but for the optimal policy among all $\\epsilon$-greedy (or $\\epsilon$-soft) policies. This is because it is an on-policy algorithm which has to balance the exploration-exploitation tradeoff. Sometimes, as a result, SARSA leads to a policy that stays further away from states with a high negative reward because it cannot safely move in the close by area. This game does not have any such states, however.\n",
    "\n",
    "Another intriguing observation about the optimal policy is the following: I already mentioned how the agent is driven by the positive reward in states close to a win while in states close to a loss, the high effort is just \"not worth it\" since a loss is quite likely in any way. Put differently, the agent avoids the additional costs incurred by taking action $HIGH$ in favor of a quick loss which does not incur any additional negative reward. If, say, a loss was associated with a negative reward of 1000, the agent would be incentivized to avoid the losing state. We would then expect the agent to choose action $HIGH$ in states close to a loss in order to escape this area as quickly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-accountability",
   "metadata": {},
   "source": [
    "### Question 2.2 - Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "specified-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(qsa, next_qs, r, alpha=0.001, gamma=1.0):\n",
    "    return qsa + alpha * (r + gamma * np.max(next_qs) - qsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "lyric-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500000/500000 [02:15<00:00, 3690.14it/s]\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((7,2))\n",
    "\n",
    "episodes = 500000\n",
    "for episode in tqdm(range(episodes)):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # act according to behavior policy\n",
    "        action = eps_greedy_policy(Q[state])\n",
    "        \n",
    "        # step environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # qlearning update\n",
    "        Q[state, action] = q_learning(Q[state, action], Q[next_state], reward)\n",
    "        \n",
    "        # set state to next state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "happy-procurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.        ]\n",
      " [ 57.26453492  29.49739819]\n",
      " [152.91018034 133.1679582 ]\n",
      " [283.55266974 269.90749708]\n",
      " [460.01745826 468.03697322]\n",
      " [691.62559894 717.26099413]\n",
      " [  0.           0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q-learning results, 500K episodes, alpha 0.001\n",
    "print(Q)\n",
    "np.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-choice",
   "metadata": {},
   "source": [
    "Like SARSA, the action values obtained though Q-learning suggest a policy that chooses action $HIGH$ whenever she has a lead over her opponent, otherwise $LOW$. This is the optimal policy, which is the expected result of this off-policy method. \n",
    "\n",
    "Indeed, unlike SARSA, Q-learning does not have to content with an approximate optimal policy. This is because the exploration-exploitation tradeoff is taken care of by using two policies - a _behavior policy_ and an _evaluation policy_. While this is more general and powerful, it is also more difficult to train. In fact, on-policy control is actually a special case of off-policy control where the behavior policy is also the evaluation policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-chinese",
   "metadata": {},
   "source": [
    "### Question 2.3 - TD($\\lambda$)\n",
    "\n",
    ">**Note**: Algorithms using an eligibility trace should set the trace to 0 in each round, otherwise rewards obtained in one episode propagate to the next, which is unwanted. The pseudocode suggested in the lecture slides does not do this. Most likely, this mistake originated in the first edition of Sutton and Barto's Reinforcement Learning book, see [here](https://stackoverflow.com/questions/29904270/eligibility-trace-reinitialization-between-episodes-in-sarsa-lambda-implementati)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "empty-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.random.choice(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "impaired-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500000/500000 [01:46<00:00, 4716.59it/s]\n"
     ]
    }
   ],
   "source": [
    "env = Game()\n",
    "\n",
    "V = np.zeros(7)\n",
    "\n",
    "# set some variables\n",
    "step_size = 0.0001\n",
    "gamma = 1\n",
    "lambda_ = 0.9\n",
    "\n",
    "episodes = 500000\n",
    "for episode in tqdm(range(episodes)):\n",
    "    \n",
    "    e = np.zeros(7) \n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # act according to behavior policy\n",
    "        action = random_policy()\n",
    "        \n",
    "        # step environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # compute td error and perfrom updates\n",
    "        td_error = reward + gamma * V[next_state] - V[state]\n",
    "        e[state] += 1\n",
    "            \n",
    "        V += step_size * e * td_error\n",
    "        e *= gamma * lambda_\n",
    "        \n",
    "        # set state to next state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "framed-bobby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,  15.16744125,  89.18443075, 224.24735204,\n",
       "       421.93389927, 680.03555275,   0.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# td lambda results, 500K episodes\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-change",
   "metadata": {},
   "source": [
    "Judging by the policy value of the starting state, we can see that the random policy would rank somewhere in the middle of all deterministic policies. In other words, a random policy would do better than roughly half of all deterministic policies, which is quite unexpected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-syndicate",
   "metadata": {},
   "source": [
    "## Part 3 - SARSA($\\lambda$)\n",
    "\n",
    "In contrast to the previous setup, there are now $7x11$ states due to the fact that there are $7$ different scores (from 0 to 6) and $11$ different energy levels (from 0 to 10). In addition, the agent now faces a constraint on the action choice when the energy level is 0. The most natural place to implement that constraint is the policy, which can be seen as the agent's strategy for taking actions. In any state with energy level 0, she simply has no other choice than to select action $LOW$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "strange-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constr_eps_greedy_policy(qs, energy, epsilon=0.1):\n",
    "    \n",
    "    if energy == 0:\n",
    "        return LOW\n",
    "    \n",
    "    if np.random.binomial(1, epsilon) == 1:\n",
    "        return np.random.choice(2) # only two actions\n",
    "    \n",
    "    else:\n",
    "        return np.random.choice([action_ for action_, value_ in enumerate(qs) if value_ == np.max(qs)])\n",
    "        # or np.argmax(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "narrow-malaysia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 500000/500000 [03:42<00:00, 2245.76it/s]\n"
     ]
    }
   ],
   "source": [
    "env = EnergyGame()\n",
    "\n",
    "Q = np.zeros((7,11,2))\n",
    "# C = np.zeros_like(Q)\n",
    "\n",
    "# set some variables\n",
    "step_size = 0.001\n",
    "gamma = 0.9\n",
    "lambda_ = 0.9\n",
    "\n",
    "episodes = 500000\n",
    "for episode in tqdm(range(episodes)):\n",
    "    \n",
    "    e = np.zeros_like(Q)\n",
    "    state = env.reset()\n",
    "    \n",
    "    # for sarsa, first action before loop\n",
    "    action = constr_eps_greedy_policy(Q[state], state[1])\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # step environment\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # get sarsa next action, on-policy\n",
    "        next_action = constr_eps_greedy_policy(Q[next_state], next_state[1])\n",
    "        \n",
    "        # compute td error\n",
    "        td_error = reward + gamma * Q[next_state][next_action] - Q[state][action]\n",
    "        e[state][action] += 1\n",
    "        \n",
    "        # update Q and eligibility trace\n",
    "        Q += step_size * e * td_error\n",
    "        e *= gamma * lambda_\n",
    "        \n",
    "        # C[state][action] +=1\n",
    "        \n",
    "        # update state and action\n",
    "        state = next_state\n",
    "        action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "sublime-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-action values after 500K episodes (dimensions are score, energy, action):\n",
      "[[[ 47.49   0.  ]\n",
      "  [ 28.96   2.53]\n",
      "  [ 32.24  65.86]\n",
      "  [ 55.99  69.49]\n",
      "  [ 56.26  76.89]\n",
      "  [ 73.06  48.7 ]\n",
      "  [ 66.42  89.95]\n",
      "  [ 20.32  80.62]\n",
      "  [ 71.12  96.2 ]\n",
      "  [ 55.7   90.25]\n",
      "  [ 70.99  90.08]]\n",
      "\n",
      " [[113.05   0.  ]\n",
      "  [ 62.06 125.85]\n",
      "  [140.52 107.08]\n",
      "  [154.71 137.37]\n",
      "  [129.72 168.5 ]\n",
      "  [151.16 178.75]\n",
      "  [132.27 169.69]\n",
      "  [157.49 193.85]\n",
      "  [188.67 155.6 ]\n",
      "  [167.43 194.32]\n",
      "  [169.73 185.26]]\n",
      "\n",
      " [[205.74   0.  ]\n",
      "  [140.44 238.73]\n",
      "  [155.2  255.72]\n",
      "  [226.75 286.  ]\n",
      "  [252.73 296.45]\n",
      "  [214.78 282.5 ]\n",
      "  [265.5  323.84]\n",
      "  [287.68 228.42]\n",
      "  [275.38 324.76]\n",
      "  [273.95 300.84]\n",
      "  [287.74 329.57]]\n",
      "\n",
      " [[361.45   0.  ]\n",
      "  [402.86 175.28]\n",
      "  [327.26 452.34]\n",
      "  [409.88 464.8 ]\n",
      "  [440.17 397.87]\n",
      "  [439.93 487.17]\n",
      "  [455.95 430.39]\n",
      "  [439.38 498.49]\n",
      "  [451.81 468.86]\n",
      "  [440.99 508.49]\n",
      "  [454.97 468.96]]\n",
      "\n",
      " [[639.59   0.  ]\n",
      "  [387.57 710.54]\n",
      "  [150.55 705.73]\n",
      "  [666.35 482.7 ]\n",
      "  [622.55 741.93]\n",
      "  [223.28 729.32]\n",
      "  [683.9  747.56]\n",
      "  [478.51 736.66]\n",
      "  [689.38 750.06]\n",
      "  [697.61 682.19]\n",
      "  [687.11 741.98]]]\n",
      "\n",
      "Suggested policy:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 1 0 1 1 1 1 1]\n",
      " [0 1 0 0 1 1 1 1 0 1 1]\n",
      " [0 1 1 1 1 1 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 1 1 1 1]\n",
      " [0 1 1 0 1 1 1 1 1 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# SARSA lambda results, 500K episodes\n",
    "print('State-action values after 500K episodes (dimensions are score, energy, action):')\n",
    "print(Q[1:6])\n",
    "print('\\nSuggested policy:')\n",
    "print(np.argmax(Q, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-bicycle",
   "metadata": {},
   "source": [
    "To begin with, as was the case with Monte Carlo in parts 1.2 and 1.3, the state-action values do not fully converge with 500K episodes. As a result, the values as well as the resulting optimal policy can change slightly from run to run.\n",
    "\n",
    "First things first, the state-action values increase with both the score and the energy level, which is expected. The optimal policy appears to suggest action $HIGH$ in most states. Especially if the energy level is high (10 or close to 10), the agent can safely take the energy loss since most episodes only last a few rounds, no matter the current state. In fact, the energy budget is quite large for the game such that states with low energy get visited only rarely, which leads to a higher variance in the estimates for those states.\n",
    "\n",
    "Due to the constraint of having to choose action $LOW$ if the energy is $0$, we would expect the agent to avoid those states. Especially if the score is low too. In other words, sometimes it makes more sense to choose action low in order to save energy for when it is needed most, which is when the agent is close to the losing state. \n",
    "\n",
    "That being said, learning the optimal policy given these parameters is not easy. First of all, the results seem to be influenced by the very first action taken in each state. This is because of the following:\n",
    "\n",
    "Say the state-action values are initialized with 0. Early on, each time an action value is updated, it will be a positive (or 0) update and that action will therefore be chosen with a high probability (i.e. $1-\\epsilon$) again in the future. Since all updates are small and incremental, it is likely that the state-action values who got updated first \"run away\" from those that did not get updated early in the game. As a result, they cannot catch up. Of course, eventually all state-action values should converge. However, the number of episodes may simply not be large enough. This is especially problematic for states that are not visited often, which is true for states with a low energy level, and even more so for states with not only a low energy level but also a low score.\n",
    "\n",
    "A potential solution to this problem would be to use optimistic initial values. In essence, the problem I just described due to insufficient exploration, especially in the early phase of the game. Optimistic initial values encourage exploration.\n",
    "\n",
    "Below is the optimal policy found after 2 Mio episodes instead of 500K. We can see that action $HIGH$ is even more prevalent except for a few states in which the energy is at level 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "female-chosen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested policy after 2 Mio episodes:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1 1 1 1]\n",
      " [0 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# SARSA lambda results, 2 Mio episodes\n",
    "#print(Q[1:6])\n",
    "print('Suggested policy after 2 Mio episodes:')\n",
    "print(np.argmax(Q, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-passing",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "#### 4.1 Winning probability of player $X$ when $x=1$ and any value of $y$.\n",
    "\n",
    "Player $X$ cannot afford to lose even a single round of the game. She must win all rounds, which is until player $Y$ has lost all her tokens. The probability of winning each round of the game is $0.5$. Therefore,\n",
    "\n",
    "$$\n",
    "P(win|x=1, y) = 0.5^y \n",
    "$$\n",
    "\n",
    "Alternatively, the probability can also be defined in a recursive fashion.\n",
    "\n",
    "$$\n",
    "P(win|x=1, y) = 0.5 \\cdot P(win|x=1, y-1) = 0.5^y \n",
    "$$\n",
    "\n",
    "\n",
    "#### 4.2 Winning probability of player $X$ when $y=1$ and any value of $x$.\n",
    "\n",
    "Clearly, player $X$ is now in the exact opposite position to before. She only loses if player $Y$ wins all rounds. For example, if $x=3$, player $X$ can occur a maximum of two losses in three games. As a result, the solution is essentially $1$ minus the previous result.\n",
    "\n",
    "$$\n",
    "P(win|y=1, x) = 1-P(loss|y=1, x) = 1-0.5^x\n",
    "$$\n",
    "\n",
    "Again, this can also be recursively defined as\n",
    "\n",
    "$$\n",
    "P(win|y=1, x) = 1-P(loss|y=1, x) = 1 - 0.5\\cdot P(loss|y=1, x-1) = 1-0.5^x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-length",
   "metadata": {},
   "source": [
    "#### 4.3 Winning probability of player $X$ for any $x$ and $y$.\n",
    "\n",
    "I present two solutions to this problem. First, a combinatorial one that is simple and allows to calculate the probabilities directly for any given $x$ and $y$. Second, I also present a solution which is based on representing the game as a Markov Reward Process (MRP).\n",
    "\n",
    "##### a) Combinatorial Solution\n",
    "The game can be seen as a combinatorial problem. The maximum number of rounds in the game is $n=x+y-1$. Playing the game is like repeatedly drawing balls (black and white) from an urn with replacement. Let us assume that $black$ means a win for player $X$ while $white$ represents a win for player $Y$. The total number of possible outcomes (where the order does not matter) is therefore $2^{n}$.\n",
    "\n",
    "The probability of winning this game (from the perspective of player $X$) is therefore the share all such combinations with at least $y$ black balls. The number of combinations with any $k$ black balls is $\\binom{n}{k}$. Therefore,\n",
    "\n",
    "$$\n",
    "P(win|x,y) = \\frac{\\text{# of combinations with at least y black balls}} {\\text{# of combinations}} = \\frac{ \\sum_{i=y}^{n} \\binom{n}{i} } {2^n}\n",
    "$$\n",
    "\n",
    "Also note that $2^n= \\sum_{i=0}^{n} \\binom{n}{i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "illegal-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb\n",
    "\n",
    "def win_probability(x,y):\n",
    "    \n",
    "    max_rounds = x+y-1\n",
    "    total_comb = 2**max_rounds\n",
    "    win_comb = sum([comb(max_rounds, i) for i in range(y, max_rounds+1)])\n",
    "    \n",
    "    print(f'x={x}, y={y}')\n",
    "    print('max', max_rounds, 'rounds')\n",
    "    print('# of combinations is', total_comb)\n",
    "    print('# of winning combinations is', win_comb)\n",
    "    print(f'P(win | x={x}, y={y}) =', win_comb/total_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "pointed-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=6, y=4\n",
      "max 9 rounds\n",
      "# of combinations is 512\n",
      "# of winning combinations is 382\n",
      "P(win | x=6, y=4) = 0.74609375\n"
     ]
    }
   ],
   "source": [
    "x = 6\n",
    "y = 4\n",
    "\n",
    "win_probability(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-belief",
   "metadata": {},
   "source": [
    "#### b) Markov Reward Process Solution\n",
    "\n",
    "The game can also be represented as a Markov Reward Process (MRP) with the following characteristics.\n",
    "\n",
    "The game starts in state $(x,y)$. From there, it proceeds according to the underlying dynamics described in the problem definition. Overall, there are $xy+1$ states (any combination of $x$ and $y$ plus a terminal state). If a win is associated with a reward of $1$ and a loss with $0$, then the resulting state values can be interpreted as the probabilities of winning the game from each state. \n",
    "\n",
    "The state value function, or Bellman equation (for an MRP) is \n",
    "\n",
    "$$\n",
    "V(s) = R(s) + \\gamma\\sum_{s'}P(s'|s)V(s'),\\,\\text{or}\\\\\\\\\n",
    "V = R+\\gamma PV\n",
    "$$\n",
    "\n",
    "where the latter is in matrix notation. As already described in **Part 1.1** of the assignment, transforming the Bellman equation leads to a closed form solution of the MRP.\n",
    "\n",
    "$$V = (I-\\gamma P)^{-1}R$$\n",
    "\n",
    "Essentially, this solution is nothing more than a system of $xy$ linear equations in $xy$ unknowns. Of course, if the state space is too large it can become computationally unfeasible to obtain the closed form solution (which includes the expensive matrix inverse). In this case, we can also fall back to the iterative policy evaluation method described and used in part $1.1$ of this assignment.\n",
    "\n",
    "Likewise to part 1.1, the solution requires the transition matrix $P$ and the vector of immediate expected rewards $R$. The latter is simply a $xy$ dimensional array with $0.5$ in any state where $y=1$ and $0$ otherwise. The states themselves are represented as a flat vector. That makes the construction of the transition matrix challenging even though conceptually, it is very simple: there is a $0.5$ chance of moving from any state to either of its two \"child\" or successor states. All other transitions cannot occur. Below is my solution using the starting state $(x=6, y=4)$.\n",
    "\n",
    "##### First, create and encode all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "alien-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set x and y\n",
    "x = 6\n",
    "y = 4\n",
    "\n",
    "# create all states\n",
    "states = np.array(list(itertools.product(range(0,x+1), range(0,y+1))))\n",
    "n = states.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "medical-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] is encoded as 0\n",
      "[0 1] is encoded as 1\n",
      "[0 2] is encoded as 2\n",
      "[0 3] is encoded as 3\n",
      "[0 4] is encoded as 4\n",
      "[1 0] is encoded as 5\n",
      "[1 1] is encoded as 6\n",
      "[1 2] is encoded as 7\n",
      "[1 3] is encoded as 8\n",
      "[1 4] is encoded as 9\n",
      "[2 0] is encoded as 10\n",
      "[2 1] is encoded as 11\n",
      "[2 2] is encoded as 12\n",
      "[2 3] is encoded as 13\n",
      "[2 4] is encoded as 14\n",
      "[3 0] is encoded as 15\n",
      "[3 1] is encoded as 16\n",
      "[3 2] is encoded as 17\n",
      "[3 3] is encoded as 18\n",
      "[3 4] is encoded as 19\n",
      "[4 0] is encoded as 20\n",
      "[4 1] is encoded as 21\n",
      "[4 2] is encoded as 22\n",
      "[4 3] is encoded as 23\n",
      "[4 4] is encoded as 24\n",
      "[5 0] is encoded as 25\n",
      "[5 1] is encoded as 26\n",
      "[5 2] is encoded as 27\n",
      "[5 3] is encoded as 28\n",
      "[5 4] is encoded as 29\n",
      "[6 0] is encoded as 30\n",
      "[6 1] is encoded as 31\n",
      "[6 2] is encoded as 32\n",
      "[6 3] is encoded as 33\n",
      "[6 4] is encoded as 34\n"
     ]
    }
   ],
   "source": [
    "# encode the states\n",
    "def index_to_state(index):\n",
    "    return np.unravel_index(index, (n,n))\n",
    "\n",
    "def state_to_index(state):\n",
    "    return np.ravel_multi_index(state, (x+1,y+1))\n",
    "\n",
    "for state in states:\n",
    "    print(state, 'is encoded as', state_to_index(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-athens",
   "metadata": {},
   "source": [
    "Note that there are several terminal states but can be considered one state.\n",
    "\n",
    "##### Second, create the transition matrix $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "inclusive-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children(state):\n",
    "    \n",
    "    child1 = state + np.array([0,-1])\n",
    "    child2 = state + np.array([-1,0])\n",
    "    \n",
    "    children = (child1, child2)\n",
    "\n",
    "    return [child for child in children if (child>=0).all()]\n",
    "\n",
    "# for example\n",
    "# get_children([4,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "virgin-legislation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.5 0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.5 0.  ... 0.  0.  0. ]\n",
      " ...\n",
      " [0.  0.  0.  ... 0.  0.  0. ]\n",
      " [0.  0.  0.  ... 0.5 0.  0. ]\n",
      " [0.  0.  0.  ... 0.  0.5 0. ]]\n"
     ]
    }
   ],
   "source": [
    "# create the transition matrix\n",
    "P = np.zeros((n,n))\n",
    "\n",
    "for state in states:\n",
    "    \n",
    "    children = get_children(state)\n",
    "    \n",
    "    for child in children:\n",
    "        state_index = state_to_index(state)\n",
    "        child_index = state_to_index(child)\n",
    "        P[state_index, child_index] = 0.5\n",
    "        \n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-negotiation",
   "metadata": {},
   "source": [
    "##### Third, create the vector of expected immediate rewards $R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cloudy-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.5 0.\n",
      " 0.  0.  0.  0.5 0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.5 0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "# create vector of expected immediate rewards\n",
    "R = np.zeros(n)\n",
    "\n",
    "for i, state in enumerate(states):\n",
    "    if state[1] == 1 and state[0] != 0:\n",
    "        R[i] = 0.5\n",
    "        \n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-funeral",
   "metadata": {},
   "source": [
    "##### Finally, solve the MRP.\n",
    "As can be seen from output below, computing the solution for a single starting state - say $(x=6, y=4)$ - requires evaluating all states of the MRP. This is because the value of one state depends on the value of its successor states (as is the case with dynamic programming). Therefore, this solution is quite expensive computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "focused-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve MDP in closed form\n",
    "V = np.linalg.inv(np.eye(n)-P).dot(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "anticipated-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(win | x=1, y=1) = 0.5\n",
      "P(win | x=1, y=2) = 0.25\n",
      "P(win | x=1, y=3) = 0.125\n",
      "P(win | x=1, y=4) = 0.0625\n",
      "P(win | x=2, y=1) = 0.75\n",
      "P(win | x=2, y=2) = 0.5\n",
      "P(win | x=2, y=3) = 0.3125\n",
      "P(win | x=2, y=4) = 0.1875\n",
      "P(win | x=3, y=1) = 0.875\n",
      "P(win | x=3, y=2) = 0.6875\n",
      "P(win | x=3, y=3) = 0.5\n",
      "P(win | x=3, y=4) = 0.34375\n",
      "P(win | x=4, y=1) = 0.9375\n",
      "P(win | x=4, y=2) = 0.8125\n",
      "P(win | x=4, y=3) = 0.65625\n",
      "P(win | x=4, y=4) = 0.5\n",
      "P(win | x=5, y=1) = 0.96875\n",
      "P(win | x=5, y=2) = 0.890625\n",
      "P(win | x=5, y=3) = 0.7734375\n",
      "P(win | x=5, y=4) = 0.63671875\n",
      "P(win | x=6, y=1) = 0.984375\n",
      "P(win | x=6, y=2) = 0.9375\n",
      "P(win | x=6, y=3) = 0.85546875\n",
      "P(win | x=6, y=4) = 0.74609375\n"
     ]
    }
   ],
   "source": [
    "# print out the results\n",
    "for state, v in zip(states, V):\n",
    "    if v != 0: # only non-terminal states can be stating states\n",
    "        print(f'P(win | x={state[0]}, y={state[1]}) =', v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvbandits",
   "language": "python",
   "name": "venvbandits"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
